[
  {
    "objectID": "notebooks/seqfish_pipeline.html",
    "href": "notebooks/seqfish_pipeline.html",
    "title": "seqFISH exploratory data analysis",
    "section": "",
    "text": "Introduction\nIn this vignette, we will demonstrate an example analysis workflow for spatial transcriptomics data collected using the seqFISH technology. The analysis will cover quality control, normalization, dimension reduction, clustering, and univariate spatial statistics. The workflow will make use of functions implemented in the Voyager R package as well as other Bioconductor packages.\n\n\nDataset\nThe data used in this vignette are described in Integration of spatial and single-cell transcriptomic data elucidates mouse organogenesis. Briefly, seqFISH was use to profile 351 genes in several mouse embryos at the 8-12 somite stage (ss). We will focus on a single biological replicate, embryo 3. The raw and processed counts and corresponding metadata are available to download from the Marioni lab. Expression matrices, segmentation data, and segmented cell vertices are provided as R objects that can be readily imported into an R environment. The data relevant to this vignette have been converted to a SFE object and are available to download here from Box.\nThe data have been added to the SFEData package on Bioconductor and will be available in the release release.\nWe will begin by downloading the data and loading it in to R.\n\nlibrary(Voyager)\nlibrary(SFEData)\nlibrary(SingleCellExperiment)\nlibrary(SpatialExperiment)\nlibrary(SpatialFeatureExperiment)\nlibrary(batchelor)\nlibrary(scater)\nlibrary(scran)\nlibrary(bluster)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(fossil)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(spdep)\nlibrary(BiocParallel)\n\ntheme_set(theme_bw())\n\n\n# Only Bioc release and above\nsfe &lt;- LohoffGastrulationData()\n#&gt; see ?SFEData and browseVignettes('SFEData') for documentation\n#&gt; loading from cache\n\nThe rows in the count matrix correspond to the 351 barcoded genes measured by seqFISH. Additionally, the authors provide some metadata, including the field of view and z-slice for each cell. We will filter the count matrix and metadata to include only cells from a single z-slice.\n\nnames(colData(sfe))\n#&gt;  [1] \"uniqueID\"                \"embryo\"                 \n#&gt;  [3] \"pos\"                     \"z\"                      \n#&gt;  [5] \"x_global\"                \"y_global\"               \n#&gt;  [7] \"x_global_affine\"         \"y_global_affine\"        \n#&gt;  [9] \"embryo_pos\"              \"embryo_pos_z\"           \n#&gt; [11] \"Area\"                    \"UMAP1\"                  \n#&gt; [13] \"UMAP2\"                   \"celltype_mapped_refined\"\n#&gt; [15] \"sample_id\"\n\n\nmask &lt;- colData(sfe)$z == 2\nsfe &lt;- sfe[,mask]\n\n\n\nQuality control\nWe will begin quality control (QC) of the cells by computing metrics that are common in single-cell analysis and store them in the colData field of the SFE object. Below, we compute the number of counts per cell. We will also compute the average and display it on the violin plot.\n\ncolData(sfe)$nCounts &lt;- colSums(counts(sfe))\navg &lt;- mean(colData(sfe)$nCounts)\n\nviolin &lt;- plotColData(sfe, \"nCounts\") +\n    geom_hline(yintercept = avg, color='red') +\n    theme(legend.position = \"top\") \n\nspatial &lt;- plotSpatialFeature(sfe, \"nCounts\", colGeometryName = \"seg_coords\")\n\nviolin + spatial\n\n\n\n\n\n\n\n\nNotably, the cells in this dataset have fewer counts than would be expected in a single-cell sequencing experiment and the cells with higher counts seem to be dispersed throughout the tissue. Fewer counts are expected in seqFISH experiments where probing for highly expressed genes may lead to optical crowding over multiple imaging rounds.\nSince the counts are collected from several fields of view, we will visualize the number of cells and total counts for each field separately.\n\npos &lt;- colData(sfe)$pos\ncounts_spl &lt;- split.data.frame(t(counts(sfe)), pos)\n\n# nCounts per FOV\ndf &lt;- map_dfr(counts_spl, rowSums, .id='pos') |&gt;\n    pivot_longer(cols=contains('embryo'), values_to = 'nCounts') |&gt;\n    mutate(pos = factor(pos, levels = paste0(\"Pos\", seq_len(length(unique(pos)))-1))) |&gt; \n    dplyr::filter(!is.na(nCounts))\n\ncells_fov &lt;- colData(sfe) |&gt; \n    as.data.frame() |&gt; \n    mutate(pos = factor(pos, levels = paste0(\"Pos\", seq_len(length(unique(pos)))-1))) |&gt; \n    ggplot(aes(pos,)) +\n    geom_bar() + \n    theme_minimal() + \n    labs(\n        x = \"\",\n        y = \"Number of cells\") + \n    theme(axis.text.x = element_text(angle = 90))\n\ncounts_fov &lt;- ggplot(df, aes(pos, nCounts)) +\n    geom_boxplot(outlier.size = 0.5) + \n    theme_minimal() + \n    labs(x = \"\", y = 'nCounts') + \n    theme(axis.text.x = element_text(angle = 90))\n\ncells_fov / counts_fov\n\n\n\n\n\n\n\n\nThere is some variability in the total number of counts in each field of view. It is not completely apparent what accounts for the low number of counts in some FOVs. For example, FOV 22 has the fewest number of cells, but comparably more counts are detected there than in regions with more cells (e.g. FOV 18).\nNext, will will compute the number of genes detected per cell, defined here as the number of genes with non-zero counts. We will again plot this metric for each FOV as is done above.\n\ncolData(sfe)$nGenes &lt;- colSums(counts(sfe) &gt; 0)\n\navg &lt;- mean(colData(sfe)$nGenes)\n\nviolin &lt;- plotColData(sfe, \"nGenes\") +\n    geom_hline(yintercept = avg, color='red') +\n    theme(legend.position = \"top\") \n\nspatial &lt;- plotSpatialFeature(sfe, \"nGenes\", colGeometryName = \"seg_coords\")\n\nviolin + spatial\n\n\n\n\n\n\n\n\nMany cells have fewer than 100 detected genes. This in part reflects that the panel of 351 probed genes was chosen to distinguish cell types at these developmental stages and that distinct cell types will likely express a small subset of the 351 genes. The authors also note that the gene panel consists of lowly expressed to moderately expressed genes. Taken together, these technical details can explain the relatively low number of counts and genes per cell.\nHere, we plot the number of genes detected per cell in each FOV.\n\ndf &lt;- map_dfr(counts_spl, ~ rowSums(.x &gt; 0), .id='pos') |&gt;\n    pivot_longer(cols = contains('embryo'), values_to = 'nGenes') |&gt;\n    mutate(pos = factor(pos, levels = paste0(\"Pos\", seq_len(length(unique(pos)))-1))) |&gt; \n    filter(!is.na(nGenes)) |&gt;\n    merge(df)\n\ngenes_fov &lt;- ggplot(df, aes(pos, nGenes)) +\n    geom_boxplot(outlier.size = 0.5) + \n    theme_bw() + \n    labs(x = \"\") + \n    theme(axis.text.x = element_text(angle = 90))\n\ngenes_fov\n\n\n\n\n\n\n\n\nThis plot mirrors the plot above for total counts. No single FOV stands out as an obvious outlier.\nThe authors have provided cell type assignments as metadata. We can assess whether the low quality cells tend to be located in a particular FOV.\n\nmeta &lt;- data.frame(colData(sfe)) \n\nmeta &lt;- meta |&gt; \n    group_by(pos) |&gt; \n    add_tally(name = \"nCells_FOV\") |&gt; \n    filter(celltype_mapped_refined %in% \"Low quality\") |&gt; \n    add_tally(name = \"nLQ_FOV\") |&gt; \n    mutate(prop_lq = nLQ_FOV/nCells_FOV) |&gt;\n    distinct(pos, prop_lq) |&gt; \n    ungroup() |&gt; \n    mutate(pos = factor(pos, levels = paste0(\"Pos\", seq_len(length(unique(pos)))-1)))\n\nprop_lq &lt;- ggplot(meta, aes(pos, prop_lq)) + \n    geom_bar(stat = 'identity' ) + \n    theme(axis.text.x = element_text(angle = 90)) \n\nprop_lq\n\n\n\n\n\n\n\n\nIt appears that FOV 26 and 31 have the largest fraction of low quality cells. Interestingly, these do not correspond to the FOVs with the largest number of cells overall.\nHere we plot nCounts vs. nGenes for each FOV.\n\ncount_vs_genes_p &lt;- ggplot(df, aes(nCounts, nGenes)) + \n  geom_point(\n    alpha = 0.5,\n    size = 1,\n    fill = \"white\"\n  ) +\n  facet_wrap(~ pos)\n\ncount_vs_genes_p \n\n\n\n\n\n\n\n\nAs in scRNA-seq, gene expression variance in seqFISH measurements is overdispersed compared to variance of counts that are Poisson distributed.\n\ngene_meta &lt;- map_dfr(counts_spl, colMeans, .id = 'pos') |&gt; \n  pivot_longer(cols = -pos, names_to = 'gene', values_to = 'mean')\n\ngene_meta &lt;- map_dfr(counts_spl, ~colVars(.x, useNames = TRUE), .id = 'pos') |&gt; \n  pivot_longer(-pos, names_to = 'gene', values_to='variance') |&gt; \n  full_join(gene_meta)\n#&gt; Joining with `by = join_by(pos, gene)`\n\nTo understand the mean-variance relationship, we compute the mean and variance for each gene among cells in tissue. As above, we will perform this calculation separately for each FOV\n\nggplot(gene_meta, aes(mean, variance)) + \n  geom_point(\n    alpha = 0.5,\n    size = 1,\n    fill = \"white\"\n  ) +\n  facet_wrap(~ pos) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  scale_x_log10() + scale_y_log10() +\n  annotation_logticks()\n\n\n\n\n\n\n\n\nThe red line represents the line \\(y = x\\), which is the mean-variance relationship that would be expected for Poisson distributed data. The data deviate from this expectation in each FOV. In each case, the variance is greater than what would be expected.\n\n\nData normalization and dimension reduction\nThe exploratory analysis above indicates the presence of batch effects corresponding to FOV. We will use a normalization scheme that is batch aware. As the SFE object inherits from the SpatialExperimentand SingleCellExperiment, classes, we can take advantage of normalization methods implemented in the scran and batchelor R packages.\nWe will first use the multiBatchNorm() function to scale the data within each batch. As noted in the documentation, the function uses median-based normalization on the ratio of the average counts between batches.\nBatch correction and dimension reduction is accomplished using fastMNN() which performs multi-sample PCA across multiple gene expression matrices to project all cells to a common low-dimensional space.\n\nsfe &lt;- multiBatchNorm(sfe, batch = pos)\nsfe_red &lt;- fastMNN(sfe, batch = pos, cos.norm = FALSE, d = 20)\n\nThe function fastMNN returns a batch-corrected matrix in the reducedDims slot of a SingleCellExperiment object. We will extract the relevant data and store them in the SFE ojbject.\n\nreducedDim(sfe, \"PCA\") &lt;- reducedDim(sfe_red, \"corrected\")\nassay(sfe, \"reconstructed\") &lt;- assay(sfe_red, \"reconstructed\") \n\nNow we will visualize the first two PCs in space. Here we notice that the PCs may show some spatial structure that correlates to biological niches of cells.\n\nspatialReducedDim(sfe, \"PCA\", ncomponents = 2, divergent = TRUE, diverge_center = 0)\n\n\n\n\n\n\n\n\nUnfortunately, FOV artifacts can still be seen.\n\n\nClustering\nMuch like in single cell analysis, we can use the batch-corrected data to cluster the cells. We will implement a graph-based clustering algorithm and plot the resulting clusters in space.\n\ncolData(sfe)$cluster &lt;- \n  clusterRows(reducedDim(sfe, \"PCA\"),\n                      BLUSPARAM = SNNGraphParam(\n                        cluster.fun = \"leiden\",\n                        cluster.args = list(\n                        resolution_parameter = 0.5,\n                        objective_function = \"modularity\")\n                        )\n              )\n\nThe plot below is colored by cluster ID and by the cell types provided by the author.\n\nplotSpatialFeature(sfe, c(\"cluster\", \"celltype_mapped_refined\"), \n                   colGeometryName = \"seg_coords\")\n\n\n\n\n\n\n\n\nThe authors have assigned cells to more types than are identified in the clustering step. In any case, the clustering results seem to recapitulate the major cell niches from the previous annotations. We can compute the Rand index using a function from the fossil package to assess the similarity between the two clustering results. A value of 1 would suggest the clustering results are identical, while a value of 0 would suggest that the results do not agree at all.\n\ng1 &lt;- as.numeric(colData(sfe)$cluster)\ng2 &lt;- as.numeric(colData(sfe)$celltype_mapped_refined)\n\nrand.index(g1, g2)\n#&gt; [1] 0.8429857\n\nThe relatively large Rand index suggests that cells are often found in the same cluster in both cases.\n\n\nUnivariate Spatial Statistics\nAt this point, we may be interested in identifying genes that exhibit spatial variability, or whose expression depends on spatial location within the tissue. Measures of spatial autocorrelation can be useful in identifyign genes that display spatial variablity. Among the most common measures are Moran’s I and Geary’s C. In the latter case, a less than 1 indicates positive spatial autocorrelation, while a value larger than 1 points to negative spatial autocorrelation. In the former case, positive and negative values of Moran’s I indicate positive and negative spatial autocorrelation, respectively.\nThese tests require a spatial neighborhood graph for computation of the statistic. There are several ways to define spatial neighbors and the findSpatialNeighbors() function wraps all of the methods implemented in the spdep package. Below, we compute a k-nearest neighborhood graph. The dist_type = \"idw\" weights the edges of the graph by the inverse distance between neighbors.\n\ncolGraph(sfe, \"knn5\") &lt;- findSpatialNeighbors(\n  sfe, method = \"knearneigh\", dist_type = \"idw\", \n  k = 5, style = \"W\")\n\nWe will also save the most variable genes for use in the computations below.\n\ndec &lt;- modelGeneVar(sfe)\nhvgs &lt;- getTopHVGs(dec, n = 100)\n\nWe use the runUnivariate() function to compute the spatial autocorrelation metrics and save the results and save them in the SFE object. The mc type for each test implements a permutation test for each statistic and relies on the nsim argument for computing a p-value for the statistic.\n\nsfe &lt;- runUnivariate(\n  sfe, type = \"geary.mc\", features = hvgs, \n  colGraphName = \"knn5\", nsim = 100, BPPARAM = MulticoreParam(2))\n\n\nsfe &lt;- runUnivariate(\n  sfe, type = \"moran.mc\", features = hvgs,\n  colGraphName = \"knn5\", nsim = 100, BPPARAM = MulticoreParam(2))\n\nsfe &lt;- colDataUnivariate(\n  sfe, type = \"moran.mc\", features = c(\"nCounts\", \"nGenes\"), \n  colGraphName = \"knn5\", nsim = 100)\n\nWe can plot the results of the Monte Carlo simulations:\n\nplotMoranMC(sfe, \"Meox1\")\n\n\n\n\n\n\n\n\nThe vertical line represents the observed value of Moran’s I and the density represents Moran’s I computed from the permuted data. These simulations suggest that the spatial autocorrelation for this feature is significant.\nThe function can also be used to plot the geary.mc results.\nNow, we might ask: which genes display the most spatial autocorrelation?\n\ntop_moran &lt;- rownames(sfe)[order(-rowData(sfe)$moran.mc_statistic_sample01)[1:4]]\n\nplotSpatialFeature(sfe, top_moran, colGeometryName = \"seg_coords\")\n\n\n\n\n\n\n\n\nIt appears that the genes with the highest spatial autocorrelation seem to have obvious expression patterns in the tissue.\nIt would be interesting to see if these genes are also differentially expressed in the clusters above. Non-spatial differential gene expression can be interrogated using the findMarkers() function implemented in the scran package and more complex methods for identifying spatially variable genes are actively being developed.\nThese analyses bring up interesting considerations. For one, it is unclear whether normalization scheme employed here effectively removes FOV batch effects. That said, there may be times where FOV differences are expected and represent biological differences, for example in the context of a tumor sample. It remains to be seen what normalization methods will perform best in these cases, and this is represents an area for research.\n\n\nSession Info\n\nsessionInfo()\n#&gt; R version 4.5.2 (2025-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.7.3\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#&gt; \n#&gt; time zone: America/Los_Angeles\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats4    stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] BiocParallel_1.44.0             spdep_1.4-1                    \n#&gt;  [3] sf_1.0-21                       spData_2.3.4                   \n#&gt;  [5] patchwork_1.3.2                 fossil_0.4.0                   \n#&gt;  [7] shapefiles_0.7.2                foreign_0.8-90                 \n#&gt;  [9] maps_3.4.3                      sp_2.2-0                       \n#&gt; [11] dplyr_1.1.4                     tidyr_1.3.1                    \n#&gt; [13] purrr_1.2.0                     bluster_1.20.0                 \n#&gt; [15] scran_1.38.0                    scater_1.38.0                  \n#&gt; [17] ggplot2_4.0.0                   scuttle_1.20.0                 \n#&gt; [19] batchelor_1.26.0                SpatialExperiment_1.20.0       \n#&gt; [21] SingleCellExperiment_1.32.0     SummarizedExperiment_1.40.0    \n#&gt; [23] Biobase_2.70.0                  GenomicRanges_1.62.0           \n#&gt; [25] Seqinfo_1.0.0                   IRanges_2.44.0                 \n#&gt; [27] S4Vectors_0.48.0                BiocGenerics_0.56.0            \n#&gt; [29] generics_0.1.4                  MatrixGenerics_1.22.0          \n#&gt; [31] matrixStats_1.5.0               SFEData_1.12.0                 \n#&gt; [33] Voyager_1.12.0                  SpatialFeatureExperiment_1.12.1\n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.5.2             bitops_1.0-9             \n#&gt;   [3] filelock_1.0.3            tibble_3.3.0             \n#&gt;   [5] R.oo_1.27.1               lifecycle_1.0.4          \n#&gt;   [7] httr2_1.2.1               edgeR_4.8.0              \n#&gt;   [9] lattice_0.22-7            MASS_7.3-65              \n#&gt;  [11] magrittr_2.0.4            limma_3.66.0             \n#&gt;  [13] rmarkdown_2.30            yaml_2.3.10              \n#&gt;  [15] metapod_1.18.0            cowplot_1.2.0            \n#&gt;  [17] DBI_1.2.3                 RColorBrewer_1.1-3       \n#&gt;  [19] ResidualMatrix_1.20.0     multcomp_1.4-29          \n#&gt;  [21] abind_1.4-8               spatialreg_1.4-2         \n#&gt;  [23] R.utils_2.13.0            RCurl_1.98-1.17          \n#&gt;  [25] TH.data_1.1-4             rappdirs_0.3.3           \n#&gt;  [27] sandwich_3.1-1            ggrepel_0.9.6            \n#&gt;  [29] irlba_2.3.5.1             terra_1.8-80             \n#&gt;  [31] units_1.0-0               RSpectra_0.16-2          \n#&gt;  [33] dqrng_0.4.1               DelayedMatrixStats_1.32.0\n#&gt;  [35] codetools_0.2-20          DropletUtils_1.30.0      \n#&gt;  [37] DelayedArray_0.36.0       tidyselect_1.2.1         \n#&gt;  [39] memuse_4.2-3              farver_2.1.2             \n#&gt;  [41] ScaledMatrix_1.18.0       viridis_0.6.5            \n#&gt;  [43] BiocFileCache_3.0.0       jsonlite_2.0.0           \n#&gt;  [45] BiocNeighbors_2.4.0       e1071_1.7-16             \n#&gt;  [47] survival_3.8-3            tools_4.5.2              \n#&gt;  [49] ggnewscale_0.5.2          Rcpp_1.1.0               \n#&gt;  [51] glue_1.8.0                gridExtra_2.3            \n#&gt;  [53] SparseArray_1.10.1        xfun_0.54                \n#&gt;  [55] EBImage_4.52.0            HDF5Array_1.38.0         \n#&gt;  [57] withr_3.0.2               BiocManager_1.30.26      \n#&gt;  [59] fastmap_1.2.0             boot_1.3-32              \n#&gt;  [61] rhdf5filters_1.22.0       digest_0.6.37            \n#&gt;  [63] rsvd_1.0.5                R6_2.6.1                 \n#&gt;  [65] wk_0.9.4                  LearnBayes_2.15.1        \n#&gt;  [67] jpeg_0.1-11               RSQLite_2.4.3            \n#&gt;  [69] R.methodsS3_1.8.2         h5mread_1.2.0            \n#&gt;  [71] data.table_1.17.8         class_7.3-23             \n#&gt;  [73] httr_1.4.7                htmlwidgets_1.6.4        \n#&gt;  [75] S4Arrays_1.10.0           pkgconfig_2.0.3          \n#&gt;  [77] scico_1.5.0               gtable_0.3.6             \n#&gt;  [79] blob_1.2.4                S7_0.2.0                 \n#&gt;  [81] XVector_0.50.0            htmltools_0.5.8.1        \n#&gt;  [83] fftwtools_0.9-11          scales_1.4.0             \n#&gt;  [85] png_0.1-8                 knitr_1.50               \n#&gt;  [87] rjson_0.2.23              coda_0.19-4.1            \n#&gt;  [89] nlme_3.1-168              curl_7.0.0               \n#&gt;  [91] proxy_0.4-27              cachem_1.1.0             \n#&gt;  [93] zoo_1.8-14                rhdf5_2.54.0             \n#&gt;  [95] BiocVersion_3.22.0        KernSmooth_2.23-26       \n#&gt;  [97] parallel_4.5.2            vipor_0.4.7              \n#&gt;  [99] AnnotationDbi_1.72.0      s2_1.1.9                 \n#&gt; [101] pillar_1.11.1             grid_4.5.2               \n#&gt; [103] vctrs_0.6.5               BiocSingular_1.26.0      \n#&gt; [105] dbplyr_2.5.1              beachmat_2.26.0          \n#&gt; [107] sfheaders_0.4.4           cluster_2.1.8.1          \n#&gt; [109] beeswarm_0.4.0            evaluate_1.0.5           \n#&gt; [111] zeallot_0.2.0             magick_2.9.0             \n#&gt; [113] mvtnorm_1.3-3             cli_3.6.5                \n#&gt; [115] locfit_1.5-9.12           compiler_4.5.2           \n#&gt; [117] rlang_1.1.6               crayon_1.5.3             \n#&gt; [119] labeling_0.4.3            classInt_0.4-11          \n#&gt; [121] ggbeeswarm_0.7.2          viridisLite_0.4.2        \n#&gt; [123] deldir_2.0-4              Biostrings_2.78.0        \n#&gt; [125] tiff_0.1-12               Matrix_1.7-4             \n#&gt; [127] ExperimentHub_3.0.0       sparseMatrixStats_1.22.0 \n#&gt; [129] bit64_4.6.0-1             Rhdf5lib_1.32.0          \n#&gt; [131] KEGGREST_1.50.0           statmod_1.5.1            \n#&gt; [133] AnnotationHub_4.0.0       igraph_2.2.1             \n#&gt; [135] memoise_2.0.1             bit_4.6.0"
  },
  {
    "objectID": "notebooks/sample_pipeline.html",
    "href": "notebooks/sample_pipeline.html",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "",
    "text": "This notebook demonstrates pre-processing and basic analysis of the 1k Human PBMCs dataset from 10x Genomics using the 10x Genomics Chromium Single Cell 3’ Next Gen v3.1 assay."
  },
  {
    "objectID": "notebooks/sample_pipeline.html#an-introductory-analysis-workflow",
    "href": "notebooks/sample_pipeline.html#an-introductory-analysis-workflow",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "An Introductory Analysis Workflow",
    "text": "An Introductory Analysis Workflow\nThere are many ways to perform a single-cell RNA sequencing (scRNA-seq) analysis and many aspects of your data to explore. In this tutorial, we outline a basic approach to quality control and analysis, along with some intuitive explanation of the underlying mathematics.\n\nThis scRNA-seq workflow has the following steps:\n\nRead Pseudoalignment\nQuality Control\nCell and Gene Filters\nNormalization and Variance Stabilization\nDimensionality Reduction\nClustering and Visualization\nDifferential Expression"
  },
  {
    "objectID": "notebooks/sample_pipeline.html#initial-set-up",
    "href": "notebooks/sample_pipeline.html#initial-set-up",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "Initial Set-Up",
    "text": "Initial Set-Up\n\nInstall Python Packages\n\n# @title\n!pip install --quiet scanpy python-igraph louvain pybiomart\n!pip install --quiet matplotlib\n!pip install --quiet scikit-learn\n!pip install --quiet numpy\n!pip install --quiet scipy\n\n!pip install --quiet kb-python==0.29.1\n\n\n# @title\n# Import packages\nimport anndata\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy import sparse, io\n\nmatplotlib.rcParams.update({'font.size': 12})\nset_matplotlib_formats('retina')\n\n/usr/local/lib/python3.12/dist-packages/scanpy/_utils/__init__.py:33: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('anndata')` instead.\n  from anndata import __version__ as anndata_version\n/usr/local/lib/python3.12/dist-packages/scanpy/__init__.py:24: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('anndata')` instead.\n  if Version(anndata.__version__) &gt;= Version(\"0.11.0rc2\"):\n/usr/local/lib/python3.12/dist-packages/scanpy/readwrite.py:16: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('anndata')` instead.\n  if Version(anndata.__version__) &gt;= Version(\"0.11.0rc2\"):\n\n\n\n\nDownload the scRNA-seq data\n\n!wget -q https://cf.10xgenomics.com/samples/cell-exp/4.0.0/SC3_v3_NextGem_SI_PBMC_CSP_1K/SC3_v3_NextGem_SI_PBMC_CSP_1K_fastqs.tar\n!tar -xf SC3_v3_NextGem_SI_PBMC_CSP_1K_fastqs.tar"
  },
  {
    "objectID": "notebooks/sample_pipeline.html#step-1-read-pseudoalignment",
    "href": "notebooks/sample_pipeline.html#step-1-read-pseudoalignment",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "Step 1: Read Pseudoalignment",
    "text": "Step 1: Read Pseudoalignment\n\nDownload a Pre-Built Index with kb ref\n\n!kb ref -d human -i index.idx -g t2g.txt\n\n[2025-10-21 04:48:16,146]    INFO [download] Skipping download because some files already exist. Use the --overwrite flag to overwrite.\n\n\n\n\nPseudoalign the scRNA-seq Data to the Index with kb count\n\n# This step runs `kb` to pseudoalign the reads, and then generate the cells x gene matrix in h5ad format.\n!kb count -i index.idx -g t2g.txt -x 10XV3 --h5ad -t 2 \\\nSC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_S1_L002_R1_001.fastq.gz \\\nSC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_S1_L002_R2_001.fastq.gz \\\nSC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_S1_L003_R1_001.fastq.gz \\\nSC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_S1_L003_R2_001.fastq.gz \\\nSC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_S1_L004_R1_001.fastq.gz \\\nSC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_fastqs/SC3_v3_NextGem_SI_CSP-Labeled_PBMCs_1K_gex_S1_L004_R2_001.fastq.gz\n\n[2025-10-21 04:48:27,652]    INFO [count] Skipping kallisto bus because output files already exist. Use the --overwrite flag to overwrite.\n[2025-10-21 04:48:27,652]    INFO [count] Sorting BUS file ./output.bus to ./tmp/output.s.bus\n[2025-10-21 04:48:46,541]    INFO [count] On-list not provided\n[2025-10-21 04:48:46,541]    INFO [count] Copying pre-packaged 10XV3 on-list to .\n[2025-10-21 04:48:47,999]    INFO [count] Inspecting BUS file ./tmp/output.s.bus\n[2025-10-21 04:49:06,258]    INFO [count] Correcting BUS records in ./tmp/output.s.bus to ./tmp/output.s.c.bus with on-list ./10x_version3_whitelist.txt\n[2025-10-21 04:49:31,255]    INFO [count] Sorting BUS file ./tmp/output.s.c.bus to ./output.unfiltered.bus\n[2025-10-21 04:49:38,691]    INFO [count] Generating count matrix ./counts_unfiltered/cells_x_genes from BUS file ./output.unfiltered.bus\n[2025-10-21 04:49:47,233]    INFO [count] Writing gene names to file ./counts_unfiltered/cells_x_genes.genes.names.txt\n[2025-10-21 04:49:47,817] WARNING [count] 13914 gene IDs do not have corresponding valid gene names. These genes will use their gene IDs instead.\n[2025-10-21 04:49:47,866]    INFO [count] Reading matrix ./counts_unfiltered/cells_x_genes.mtx\n[2025-10-21 04:49:48,887]    INFO [count] Writing matrix to h5ad ./counts_unfiltered/adata.h5ad\n\n\nWhen the --h5ad argument is used, kb count generates a H5AD-formatted Anndata matrix for downstream processing.\n\n\nLoad the Anndata Object\nFor this tutorial we will be using the Python package Scanpy.\nScanpy is a scalable Python library for analyzing single-cell RNA sequencing (scRNA-seq) data, built around an efficient AnnData object that stores expression matrices and associated metadata. It provides a comprehensive toolkit for preprocessing, visualization, clustering, and differential expression, enabling end-to-end analysis of large single-cell datasets.\nBelow we load the cells x genes matrix generated by kb-python as a H5AD-formatted object so that we can analyze our data with Scanpy.\n\n# import data\nadata = anndata.read_h5ad('counts_unfiltered/adata.h5ad')\nadata\n\nAnnData object with n_obs × n_vars = 281262 × 39546\n\n\n\n# Load the gene names and make them a column in the var dataframe\nwith open(\"counts_unfiltered/cells_x_genes.genes.names.txt\", 'r') as file:\n  adata.var['gene_names'] = file.readlines()"
  },
  {
    "objectID": "notebooks/sample_pipeline.html#step-2-basic-quality-control",
    "href": "notebooks/sample_pipeline.html#step-2-basic-quality-control",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "Step 2: Basic Quality Control",
    "text": "Step 2: Basic Quality Control\n\nTest for Library Saturation\nBelow is a Gene vs. UMI plot, in which, for each cell, we visualize how many genes we detected. The purpose of this plot is to see if we have “saturated” our sequencing library and sampled a representative number of mRNA molecules for each cell. If this is the case, the curve should plateau as we sequence more UMIs.\n\n# Create a plot showing genes detected as a function of UMI counts.\nfig, ax = plt.subplots(figsize=(10, 7))\n\nx = np.asarray(adata.X.sum(axis=1))[:,0]\ny = np.asarray(np.sum(adata.X&gt;0, axis=1))[:,0]\n\nax.scatter(x, y, color=\"green\", alpha=0.25)\nax.set_xlabel(\"UMI Counts\")\nax.set_ylabel(\"Genes Detected\")\nax.set_xscale('log')\nax.set_yscale('log', nonpositive='clip')\n\nax.set_xlim((0.5, 4500))\nax.set_ylim((0.5,2000))\n\n\nplt.show()\n\n\n\n\n\n\n\n\nThe number of genes detected continues to increase with the number of UMI counts (x-axis) indicating that we have not saturated our library. More sequencing would likely yield more genes detected per cell.\n\n\nExamine the Knee Plot\nIn microfluidics-based scRNA-seq assays, such as the 10x Genomics Chromium platform, cells are encapsulated into droplets at low concentrations to minimize the formation of multiplets (droplets containing more than one cell). However, this approach also results in many droplets that contain no cells at all.\nDuring cell dissociation and droplet generation, some mRNA molecules are released into the surrounding solution — known as ambient RNA. When this ambient RNA is captured and sequenced, empty droplets may falsely appear to contain a cell.\nThe knee plot helps distinguish real cells from empty droplets based on total UMI counts per droplet. It was first introduced in the Drop-seq paper by Macosko et al., 2015.\nIn this plot, barcodes (droplets) are ranked by the number of associated UMI counts (shown on the x-axis), while the fraction of droplets with at least that many UMIs is shown on the y-axis. The “knee” of the curve represents the point where UMI counts drop sharply — separating droplets that likely contain real cells from those containing only ambient RNA.\n\n#@title Threshold Cells According to Knee Plot { run: \"auto\", vertical-output: true }\ncutoff = 400  #@param {type:\"integer\"}\nknee = np.sort((np.array(adata.X.sum(axis=1))).flatten())[::-1]\ncell_set = np.arange(len(knee))\nnum_cells = cell_set[knee &gt; cutoff][::-1][0]\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\n\nax.loglog(knee, cell_set, linewidth=5, color=\"g\")\nax.axvline(x=cutoff, linewidth=3, color=\"k\")\n\n\nax.axhline(y=num_cells, linewidth=3, color=\"k\")\n\nax.set_xlabel(\"UMI Counts\")\nax.set_ylabel(\"Set of Barcodes\")\n\nplt.grid(True, which=\"both\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"{num_cells:,.0f} cells passed the {cutoff} UMI threshold\")\n\n1,212 cells passed the 400 UMI threshold\n\n\nThe knee plot can be used to threshold cells based on the number of UMI counts they contain. In this example we use the number 3979 based on the publication describing the data."
  },
  {
    "objectID": "notebooks/sample_pipeline.html#step-3-doublet-and-multiplet-removal",
    "href": "notebooks/sample_pipeline.html#step-3-doublet-and-multiplet-removal",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "Step 3: Doublet and Multiplet Removal",
    "text": "Step 3: Doublet and Multiplet Removal\n\nFilter empty droplets (According to the Knee Plot)\nIdeally, your data will contain no empty droplets that can be mistaken for real cells. We use our results from the knee plot here to filter them out.\n\n# Filter the cells according to the threshold determined from the knee plot\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_cells(adata, min_counts=knee[num_cells])\n\n\n\n\n\n\n\nNote\n\n\n\nThe knee plot is a very simple method for getting rid of empty droplets. More sophisticated methods also exist. For instance, the methods EmptyDrops, CellBender, and DecontX use a statistical model to distinguish counts from ambient vs. cellular mRNA. This allows for both the identification and removal of empty droplets and the removal of technical noise from your count data.\n\n\n\n\nFilter Multiplets with Scrublet\nScrublet is an package designed to identify and remove multiplets (droplets containing more than one cell) from scRNA-seq data. It works by simulating artificial doublets through random pairwise combinations of real cells’ expression profiles. Scrublet then embeds both the simulated and observed cells in a low-dimensional space and flags real cells that lie close to simulated doublets as potential multiplets.\nScrublet operates on a raw count matrix, but requires that empty droplets be removed beforehand. If empty droplets remain in the dataset, they can distort the simulated doublet distribution and lead to inaccurate classifications. In addition, the Scrublet package should be applied to each sample/batch individually to avoid batch effects distorting the simulated doublet distribution.\nRemoving multiplets is an important quality-control step, as doublets can appear as hybrid cell types and obscure true biological variation during clustering and downstream analyses.\n\n\n\n\n\n\nNote\n\n\n\nThe Scrublet package is convenient because it is built-in to Scanpy, but it is not the only nor the most accurate doublet removal method. For a review of doublet detection methods, refer to the paper Benchmarking computational doublet-detection methods for single-cell RNA sequencing data by Nan Miles Chi and Jingyi Jessica Li.\n\n\n\nsc.pp.scrublet(adata, expected_doublet_rate=0.008)\n\n/usr/local/lib/python3.12/dist-packages/scanpy/neighbors/__init__.py:430: FutureWarning: Use obsm (e.g. `k in adata.obsm` or `adata.obsm.keys() | {'u'}`) instead of AnnData.obsm_keys, AnnData.obsm_keys is deprecated and will be removed in the future.\n  if \"X_diffmap\" in adata.obsm_keys():\n\n\nThe expected doublet rate depends upon your assay and the number of cells targeted. Refer to the documentation for you specific technology to get this number. We got the expected double rate 0.008 for 1,000 cells for 10x Chromium NextGem v3.1 here.\n\nadata = adata[adata.obs['predicted_doublet'] == False]\n\n\n\nFiltering Cells by Mitochondrial Content\nA healthy, intact cell should have no miRNA in the cytoplasm. As such, a high percentage of miRNA in the cytosol indicates cellular damage. We will identify the number of miRNA in each cell and remove cells with a certain percentage of miRNA.\n\n# Identify mitochondrial genes based on human-specific mtRNA prefix\nadata.var['is_mito'] = adata.var['gene_names'].str.startswith(\"MT-\")\n\nmito_counts = adata[:, adata.var['is_mito']].X.sum(axis=1)\n\n# Calculate total counts per cell\ntotal_counts = adata.X.sum(axis=1)\n\n# Calculate percent mitochondrial gene expression per cell\nadata.obs['percent_mito'] = np.array(mito_counts / total_counts * 100).flatten()\n\nadata.obs['n_counts'] = adata.X.sum(axis=1).A1\n\n/tmp/ipython-input-2771333037.py:8: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n  adata.var['is_mito'] = adata_base_var_names.isin(mito_genes_base)\n\n\n\nsc.pl.scatter(adata, x='n_counts', y='percent_mito')\n\n/usr/local/lib/python3.12/dist-packages/scanpy/plotting/_anndata.py:397: FutureWarning: Use obs (e.g. `k in adata.obs` or `str(adata.obs.columns.tolist())`) instead of AnnData.obs_keys, AnnData.obs_keys is deprecated and will be removed in the future.\n  if key in adata.obs_keys():\n\n\n\n\n\n\n\n\n\nWe see above that the highest density of cells appear to have less than 30% miRNA content, so we will set our threshold to 30%\n\nadata = adata[adata.obs.percent_mito &lt; 30]\n\n\n\nFilter out genes that are not present in any cells\nIn general, we also want to get rid of genes with a low cell count to simplify our data.\n\nsc.pp.filter_genes(adata, min_cells=3)\n\n/usr/local/lib/python3.12/dist-packages/scanpy/preprocessing/_simple.py:293: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n  adata.var[\"n_cells\"] = number\n\n\n\n\nVisualizing count distributions\nExamination of the gene count and UMI count distributions is useful QC to evaluate the quality of the library and how deeply it was sequenced.\n\nsc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'], jitter=0.4, multi_panel=True)"
  },
  {
    "objectID": "notebooks/sample_pipeline.html#step-4-normalization-and-variance-stabilization",
    "href": "notebooks/sample_pipeline.html#step-4-normalization-and-variance-stabilization",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "Step 4: Normalization and Variance Stabilization",
    "text": "Step 4: Normalization and Variance Stabilization\nIn an scRNA-seq experiment, UMI counts provide a relative estimate of transcript abundance within each cell. During library preparation and sequencing, mRNA molecules from different cells are reverse transcribed and sequenced to varying degrees. As a result, UMI counts cannot be interpreted as absolute measurements of RNA abundance.\nTo make counts comparable across cells, we normalize the data so that each cell has a total of 1×10⁶ counts. This scaling preserves integer values while allowing gene expression levels to be expressed as relative abundances within each cell.\nDownstream analyses such as PCA and differential expression typically assume that gene expression values are approximately continuous and have roughly constant variance across their range. However, raw scRNA-seq counts are overdispersed — they follow a negative binomial–like distribution, where the variance increases with the mean. To stabilize the variance and make the data more compatible with these methods, we apply a log transformation to the normalized counts.\n\n\n\n\n\n\nNote\n\n\n\nThis is a simplified approach for addressing overdispersion. Although log normalization is widely used, it does entirely produce constant-variance data. More sophisticated methods (e.g., variance-stabilizing transformations or model-based approaches) can better correct for overdispersion, but no perfect method currently exists — and those techniques are beyond the scope of this tutorial.\n\n\n\nsc.pp.normalize_total(adata, target_sum=1e6)\n\n\nsc.pp.log1p(adata)\n\n\nIdentify Highly Variable Genes\nTo simplify our data and reduce its dimensionality for downstream analyses, we limit our dataset to highly variable genes. Genes with low expression variance — such as housekeeping genes — contribute little to distinguishing between cell types or states, and can be excluded without loss of biological signal.\nHere, we retain only genes with a mean expression between 0.01 and 8 (values above 8 likely indicate technical artifacts) and a minimum normalized dispersion of 1. Cells are grouped (binned) by mean gene expression, and genes within each bin are filtered based on their normalized dispersion values.\n\n\n\n\n\n\nNote\n\n\n\nThe sc.pp.highly_variable_genes function in Scanpy expects input data that have already been normalized and variance-stabilized.\n\n\n\nsc.pp.highly_variable_genes(adata, min_mean=0.01, max_mean=8, min_disp=1, n_bins=20, flavor=\"seurat\")\nsc.pl.highly_variable_genes(adata)\n\n\n\n\n\n\n\n\nFinally, we scale the the data to unit variance and zero mean to standardize gene expression by preventing highly expressed genes from dominating downstream analysis and ensuring all genes are weighted equally.\n\nsc.pp.scale(adata, max_value=10)\n\n/usr/lib/python3.12/functools.py:912: UserWarning: zero-centering a sparse array/matrix densifies it.\n  return dispatch(args[0].__class__)(*args, **kw)"
  },
  {
    "objectID": "notebooks/sample_pipeline.html#step-5-clustering-and-visualization",
    "href": "notebooks/sample_pipeline.html#step-5-clustering-and-visualization",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "Step 5: Clustering and Visualization",
    "text": "Step 5: Clustering and Visualization\nWhen analyzing scRNA-seq data, clustering is used to identify distinct cell types and states. Subsequently, differential expression analysis can reveal the genes that define these cellular states.\nThere are many algorithms for clustering cells, and while they have been compared in detail in various benchmarks (see e.g., Duo et al. 2018), there is no univerally agreed upon method. Here we demonstrate clustering using Louvain clustering, which is a popular method for clustering single-cell RNA-seq data.\n\n\n\n\n\n\nNote\n\n\n\nLouvain clustering is a non-deterministic algorithm, meaning it can produce slightly different results across runs. The number and size of clusters depend on a resolution parameter, which controls the granularity of the clustering. There is no universally optimal way to choose this parameter — it typically depends on how specific or fine-grained you want your cell populations to be.\n\n\n\nsc.pp.pca(adata, mask_var=\"highly_variable\")\n\n/usr/local/lib/python3.12/dist-packages/scanpy/preprocessing/_pca/__init__.py:245: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('anndata')` instead.\n  Version(ad.__version__) &lt; Version(\"0.9\")\n\n\n\n# Cluster the cells using Louvain clustering\nsc.pp.neighbors(adata, n_neighbors=30, n_pcs=10, knn=True)\nsc.tl.louvain(adata, resolution = 1.0)\n\n/usr/local/lib/python3.12/dist-packages/scanpy/neighbors/__init__.py:430: FutureWarning: Use obsm (e.g. `k in adata.obsm` or `adata.obsm.keys() | {'u'}`) instead of AnnData.obsm_keys, AnnData.obsm_keys is deprecated and will be removed in the future.\n  if \"X_diffmap\" in adata.obsm_keys():\n\n\nIt is useful to revisit the PCA projection with points colored by cluster. Previously we computed the PCA projection directly; here we use a function in Scanpy which does the same.\n\nVisualization with PCA\n\n# Perform PCA and plot the projection to the first two dimensions, with points colored according to the Louvain clusters.\nfig, ax = plt.subplots(figsize=(10, 7))\nsc.pl.pca(adata, color='louvain', ax=ax)\n\n\n\n\n\n\n\n\n\n\nNonlinear Visualization\nThe PCA representation results from a linear projection of the data from its original high-dimensional space to a lower-dimensional one (in this case, 2D). Such projections are valuable because they preserve global structure and are mathematically well-defined and reproducible.\nIn contrast, non-linear dimensionality reduction methods—the most popular being t-SNE and UMAP—can reveal complex, curved relationships in the data that linear methods cannot capture. However, they distort global distances and relationships, depend heavily on algorithmic parameters and random initialization, and may produce different results across runs. As a result, they are excellent for visualization but should be interpreted cautiously and not used for quantitative downstream analysis.\nTo read more, please see the Pachter Lab’s paper on the drawbacks of non-linear dimensionality reduction methods.\n\nt-SNE\n\n# Visualize cells with t-SNE. The n_pcs parameter sets the number of principal components to project to prior to\n# performing t-SNE\nsc.tl.tsne(adata, n_pcs=10)\nfig, ax = plt.subplots(figsize=(10, 7))\nsc.pl.tsne(adata, color='louvain', ax=ax)\n\n\n\n\n\n\n\n\n\n\nUMAP\n\n%%time\nsc.tl.umap(adata)\nfig, ax = plt.subplots(figsize=(10, 7))\nsc.pl.umap(adata, color='louvain', ax=ax)\n\n\n\n\n\n\n\n\nCPU times: user 3.07 s, sys: 7.18 ms, total: 3.08 s\nWall time: 2.97 s"
  },
  {
    "objectID": "notebooks/sample_pipeline.html#step-6-differential-analysis",
    "href": "notebooks/sample_pipeline.html#step-6-differential-analysis",
    "title": "\nIntroductory scRNA-seq Analysis with kb-python and Scanpy\n",
    "section": "Step 6: Differential Analysis",
    "text": "Step 6: Differential Analysis\nFrom our clustering of cell states, we can perform differential expression analysis to identify marker genes that distinguish one cell state or cluster from another. This is typically done using simple statistical tests — such as the Wilcoxon rank-sum test or the t-test — which compare gene expression levels between clusters, or alternatively by fitting a more complex statistical model that accounts for variability across cells. By examining which genes are upregulated in each cluster, we can annotate clusters with known cell identities or infer novel cellular subpopulations.\nBelow, we use the Wilcoxon rank-sum test to identify marker genes. Because we are performing multiple hypothesis tests on the same dataset, we must adjust our p-values to control the false discovery rate. Several correction methods exist, but here we apply the Benjamini–Hochberg procedure, a widely used approach for multiple testing correction.\n\n\n\n\n\n\nNote\n\n\n\nStatistical tests such as the Wilcoxon rank-sum test and the t-test assume that the groups being compared were defined independently of the data. However, in single-cell analysis, clustering algorithms like Louvain define groups based on the data itself — by minimizing within-cluster variation and maximizing between-cluster separation. This violates the independence assumption, meaning these tests are not strictly valid. Despite this, they remain common in practice and can still provide useful exploratory insights when interpreted cautiously.\n\n\n\nsc.tl.rank_genes_groups(adata, gene_symbols='gene_names', groupby='louvain', method='wilcoxon')\n\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n/usr/local/lib/python3.12/dist-packages/scanpy/tools/_rank_genes_groups.py:482: RuntimeWarning: invalid value encountered in log2\n  self.stats[group_name, \"logfoldchanges\"] = np.log2(\n\n\nThe below plot shows the 10 most highly expressed genes in each cluster (compared to the other clusters) according to the Wilcoxon z-score.\n\nsc.pl.rank_genes_groups(adata, n_genes=10, gene_symbols='gene_names', groupby='louvain', method='wilcoxon')"
  },
  {
    "objectID": "pages/packages/scanpy.html",
    "href": "pages/packages/scanpy.html",
    "title": "Scanpy and the AnnData Object",
    "section": "",
    "text": "Scanpy is a widely used Python-based toolkit for analyzing single-cell RNA-seq data.\nIt provides a full analysis ecosystem, including tools for:\nScanpy is designed for large datasets through use of the AnnData data structure, allowing efficient storage and processing of millions of cells."
  },
  {
    "objectID": "pages/packages/scanpy.html#what-is-anndata",
    "href": "pages/packages/scanpy.html#what-is-anndata",
    "title": "Scanpy and the AnnData Object",
    "section": "What Is AnnData?",
    "text": "What Is AnnData?\nAnnData (“Annotated Data”) is a data structure designed to store single-cell omics datasets.\nIt organizes all parts of an scRNA-seq experiment—raw counts, metadata, embeddings, clustering labels, etc.- into a single, compact object.\nScanpy uses AnnData as its core data container.\nAlmost every Scanpy function takes an AnnData object as input and writes results back into it. This keeps your entire workflow contained in one place."
  },
  {
    "objectID": "pages/packages/scanpy.html#explore-the-anndata-object",
    "href": "pages/packages/scanpy.html#explore-the-anndata-object",
    "title": "Scanpy and the AnnData Object",
    "section": "Explore the AnnData Object",
    "text": "Explore the AnnData Object\n\nThe AnnData Structure at a Glance\nEvery AnnData object contains the following major components:\n\n\n\nAttribute\nDescription\n\n\n\n\nadata.X\nExpression matrix (cells × genes)\n\n\nadata.obs\nCell-level metadata (Pandas Dataframe)\n\n\nadata.var\nGene-level metadata (Pandas Dataframe)\n\n\nadata.layers\nAdditional expression matrices\n\n\nadata.obsm\nMulti-dimensional cell data (e.g., PCA, UMAP)\n\n\nadata.varm\nMulti-dimensional gene data (e.g., loadings)\n\n\nadata.uns\nUnstructured metadata (e.g., clustering results)\n\n\n\nThis structure allows Scanpy to store every part of your analysis inside the same object.\n\n\nAccessing and Modifying Data in an AnnData Object\nAnnData objects behave like structured containers, and you can access or modify their contents using standard Python indexing and assignment. For example, you can retrieve cell-level metadata from adata.obs, gene-level metadata from adata.var, or expression values from adata.X using familiar pandas- and NumPy-like operations. Adding new information is just as simple: assign a new column to adata.obs (e.g., adata.obs[\"sample\"] = \"A\") or create new layers or embeddings (e.g., adata.layers [\"counts\"] = adata.X.copy() or adata.obsm[\"X_umap\"] = umap_coords). Because AnnData enforces consistent dimensions, any added variables must match the number of cells or genes. This makes it easy to integrate new annotations, computed metrics, or analysis results directly into the same object that holds your expression data.\n\n\nMerging Anndata Objects\nMerging AnnData objects is useful when combining datasets from different samples, batches, or experimental conditions. The most common approach is to use anndata.concat(), which stacks objects along the observations (cells) or variables (genes) while automatically aligning shared features. For example, anndata.concat([adata1, adata2], join=\"outer\") merges two datasets and keeps the union of all genes, filling in missing values where needed. You can also merge only cells with matching gene sets using join=\"inner\". Sample-level identifiers—such as batch labels—can be added automatically with the label argument (e.g., label=\"batch\"). Because AnnData ensures that dimensions stay consistent, concatenation keeps your metadata synchronized, making it straightforward to integrate multiple datasets before downstream analysis.\n\n\nSaving and Loading AnnData Objects\nAnnData objects are saved in the HDF5-based .h5ad format, which is efficient and portable.\n\nSave your dataset:\nadata.write(\"dataset.h5ad\")\n\n\nLoad it later:\nadata = ad.read_h5ad(\"dataset.h5ad\")"
  },
  {
    "objectID": "pages/contact_us.html",
    "href": "pages/contact_us.html",
    "title": "Contact Us",
    "section": "",
    "text": "Name \nEmail \nMessage\n\n\nSend Message"
  },
  {
    "objectID": "pages/contact_us.html#still-have-questions-were-here-to-help",
    "href": "pages/contact_us.html#still-have-questions-were-here-to-help",
    "title": "Contact Us",
    "section": "",
    "text": "Name \nEmail \nMessage\n\n\nSend Message"
  },
  {
    "objectID": "pages/seqspec/seqspec_tutorials.html",
    "href": "pages/seqspec/seqspec_tutorials.html",
    "title": "Choose an Assay to Begin Your seqspec Tutorial",
    "section": "",
    "text": "10x Genomics Next GEM v3\n    \n    \n      \n      10x Genomics GEM X v4\n    \n  \n\n  \n  \n    \n      \n      Parse Evercode WT v2 (Single Index)\n    \n    \n      \n      Parse Evercode WT v2 (Dual Index)\n    \n    \n      \n      Parse Evercode WT v3"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v3.html",
    "href": "pages/seqspec/technologies/Parse/parse_v3.html",
    "title": "seqspec Tutorial: Parse Evercode WT v3 Dual-Indexed Library",
    "section": "",
    "text": "Follow one of the following links to download a template seqspec according to the Parse WT kit that you used to generate your sequencing library:\n\nParse Evercode WT Mini\nParse Evercode WT\nParse Evercode WT Mega\n\nThen, follow this tutorial to tailor the seqspec to your dataset."
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v3.html#summmary-of-library-structure-and-sequencing",
    "href": "pages/seqspec/technologies/Parse/parse_v3.html#summmary-of-library-structure-and-sequencing",
    "title": "seqspec Tutorial: Parse Evercode WT v3 Dual-Indexed Library",
    "section": "Summmary of Library Structure and Sequencing",
    "text": "Summmary of Library Structure and Sequencing\n\n\n\n\n\nAbove is a diagram of the Parse Evercode WT v3 Dual-Indexed Library. The library is structured as follows from 5’ to 3’:\n\n\n\nSequence\nType\nLength\n\n\n\n\nIllumina P5 Primer\nFixed\n29 bp\n\n\nIllumina i5 Index\nFixed\n8 bp\n\n\nTruseq Read 1\nFixed\n29 bp\n\n\ncDNA\nRandom\nVariable\n\n\nRound 1 Barcode\nOnList\n8 bp\n\n\nLinker\nFixed\n12 bp\n\n\nRound 2 Barcode\nOnList\n8 bp\n\n\nLinker\nFixed\n12 bp\n\n\nRound 3 Barcode\nOnList\n8 bp\n\n\nUMI\nRandom\n10 bp\n\n\nTruseq Read 2\nFixed\n34 bp\n\n\nIllumina i7 Index\nFixed\n8 bp\n\n\nIllumina P7 Primer\nFixed\n24 bp\n\n\n\n\nParse Biosciences recommends that the libary be sequenced in the following way:\n\n\n\n\n\n\n\n\n\n\nName\nPrimer\nCycles\nStrand\nCoverage\n\n\n\n\nRead 1\nTruseq Read 1\n64\npositive\ncDNA\n\n\nIndex 1\nTruseq Read 2\n8\npositive\nIllumina i7 Index\n\n\nIndex 2\nTruseq Read 1\n8\nnegative\nIllumina i5 Index\n\n\nRead 2\nTruseq Read 2\n58\nnegative\nUMI + Barcodes + Linkers"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v3.html#step-1-fill-in-the-assay-info",
    "href": "pages/seqspec/technologies/Parse/parse_v3.html#step-1-fill-in-the-assay-info",
    "title": "seqspec Tutorial: Parse Evercode WT v3 Dual-Indexed Library",
    "section": "Step 1: Fill in the Assay Info",
    "text": "Step 1: Fill in the Assay Info\nFill in the missing assay info according to the technology you used to sequence your library. For instance, if you used the Illumina NextSeq 2000 sequencer with the P3 reagent kit, your assay info section would look like:\n!Assay\nseqspec_version: 0.3.0\nassay_id: Parse-Evercode-WT-v3\nname: Dual-Indexed Parse Evercode WT v3\ndoi: https://support.parsebiosciences.com/hc/en-us/article_attachments/31841872776724\ndate: 2024-02-20\ndescription: Parse Evercode WT v3 using dual Illumina multiplex index\nmodalities: rna\nlib_struct: ##TBD##\nsequence_protocol: Illumina NextSeq 2000\nsequence_kit: Illumina NextSeq 2000 P3 XLEAP-SBS Reagent kit\nlibrary_protocol: Parse Evercode WT v3\nlibrary_kit: Parse Evercode WT v3"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v3.html#step-2-alter-the-sequence-spec",
    "href": "pages/seqspec/technologies/Parse/parse_v3.html#step-2-alter-the-sequence-spec",
    "title": "seqspec Tutorial: Parse Evercode WT v3 Dual-Indexed Library",
    "section": "Step 2: Alter the Sequence Spec",
    "text": "Step 2: Alter the Sequence Spec\n\nUpdate the Read Objects\nParse recommends that you sequence Read 1 to at least 64 nucleotides, but you may have decided to sequence to up to 150 nucleotides. If that is the case, you can modify the length of Read 1 by changing the ‘min_len’ and ‘max_len’ to reflect the read length. For example, if you want to sequence 150 nucleotides, upate the read as follows:\n- !Read\n  read_id: read_1\n  name: Read 1\n  modality: rna\n  primer_id: truseq_read1\n  min_len: 150\n  max_len: 150\n  strand: pos\n  files:\n  - !File\n    ...\nYou may have also decided not to sequence the sample indices if you did not sequence multiple samples at once. In this case, you should remove the Index 1 and Index 2 read objects.\n\n\nAdd in the file objects\nNext, for each read, you should fill out a file object referencing the corresponding file.\nA file object in seqspec if formatted as follows:\n- !File\n      file_id: \n      filename:\n      filetype: \n      filesize: \n      url: \n      urltype: \n      md5: \nfile_id, filetype, filename — identifies and describes the file\nfilesize - the size of the file\nurl - (optional) the link to the file\nurltype - (optional) the type of url\nmd5 - (optional) MD5 fingerprint of data\n\n\n\n\n\n\nNoteWhat does md5 stand for?\n\n\n\nAn MD5 checksum is a short, unique string of letters and numbers generated from a file’s contents. It acts like a digital fingerprint — if even a single byte in the file changes, the checksum will be different.\nIncluding the MD5 value for your files allows others to verify data integrity after download or transfer, ensuring that the file has not been corrupted or altered.\nTo generate an MD5 checksum string, type into your command-line:\nmd5sum &lt;filepath&gt; # For Linux\n\nmd5 &lt;filepath&gt; # For macOS\n\nGet-FileHash -Algorithm MD5 -Path &lt;filepath&gt; # For Windows"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v3.html#step-3-alter-the-library-spec",
    "href": "pages/seqspec/technologies/Parse/parse_v3.html#step-3-alter-the-library-spec",
    "title": "seqspec Tutorial: Parse Evercode WT v3 Dual-Indexed Library",
    "section": "Step 3: Alter the Library Spec",
    "text": "Step 3: Alter the Library Spec\nYou can modify the min_len and max_len values for the cDNA region to reflect your desired read length. For example, if you want to sequence 150 nucleotides of cDNA, update the region as follows:\n    - !Region\n        parent_id: rna\n        region_id: cdna\n        region_type: cdna\n        name: cDNA\n        sequence_type: random\n        sequence: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        min_len: 150\n        max_len: 150\n        onlist: null\n        regions: null\nAdjust the number of X characters in the sequence field to match the new length, and then adjust the full library sequence and length accordingly.\nlibrary_spec:\n- !Region\n  region_id: rna\n  region_type: named\n  name: rna\n  sequence_type: joined\n  sequence: AATGATACGGCGACCACCGAGATCTACACNNNNNNNNTCTTTCCCTACACGACGCTCTTCCGATCTXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXNNNNNNNNGAGGTGGTTGGANNNNNNNNCTGACCCCTCATNNNNNNNNNNNNNNNNNNAGATCGGAAGAGCACACGTCTGAACTCCAGTCACNNNNNNNNATCTCGTATGCCGTCTTCTGCTTG\n  min_len: 342\n  max_len: 342\n  onlist: null\n  parent_id: null\n  regions:\n  - !Region\n  ...\n\nNow you have a complete seqspec file for your data. All that’s left is to save it with your dataset!"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_single.html",
    "href": "pages/seqspec/technologies/Parse/parse_v2_single.html",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Single-Indexed Library",
    "section": "",
    "text": "Follow one of the following links to download a template seqspec according to the Parse WT kit that you used to generate your sequencing library:\n\nParse Evercode WT Mini\nParse Evercode WT\nParse Evercode WT Mega\n\nThen, follow this tutorial to tailor the seqspec to your dataset."
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_single.html#summmary-of-library-structure-and-sequencing",
    "href": "pages/seqspec/technologies/Parse/parse_v2_single.html#summmary-of-library-structure-and-sequencing",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Single-Indexed Library",
    "section": "Summmary of Library Structure and Sequencing",
    "text": "Summmary of Library Structure and Sequencing\n\n\n\n\n\nAbove is a diagram of the Parse Evercode WT v2 Single-Indexed Library. The library is structured as follows from 5’ to 3’:\n\n\n\nSequence\nType\nLength\n\n\n\n\nIllumina P5 Primer\nFixed\n29 bp\n\n\nTruseq Read 1\nFixed\n29 bp\n\n\ncDNA\nRandom\nVariable\n\n\nRound 1 Barcode\nOnList\n8 bp\n\n\nLinker\nFixed\n22 bp\n\n\nRound 2 Barcode\nOnList\n8 bp\n\n\nLinker\nFixed\n30 bp\n\n\nRound 3 Barcode\nOnList\n8 bp\n\n\nUMI\nRandom\n10 bp\n\n\nTruseq Read 2\nFixed\n34 bp\n\n\nIllumina i7 Index\nFixed\n6 bp\n\n\nIllumina P7 Primer\nFixed\n24 bp\n\n\n\n\nParse Biosciences recommends that the libary be sequenced in the following way:\n\n\n\n\n\n\n\n\n\n\nName\nPrimer\nCycles\nStrand\nCoverage\n\n\n\n\nRead 1\nTruseq Read 1\n74\npositive\ncDNA\n\n\nIndex 1\nTruseq Read 2\n6\npositive\nIllumina i7 Index\n\n\nRead 2\nTruseq Read 2\n86\nnegative\nUMI + Barcodes + Linkers"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_single.html#step-1-fill-in-the-assay-info",
    "href": "pages/seqspec/technologies/Parse/parse_v2_single.html#step-1-fill-in-the-assay-info",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Single-Indexed Library",
    "section": "Step 1: Fill in the Assay Info",
    "text": "Step 1: Fill in the Assay Info\nFill in the missing assay info according to the technology you used to sequence your library. For instance, if you used the Illumina NextSeq 2000 sequencer with the P3 reagent kit, your assay info section would look like:\n\n!Assay\nseqspec_version: 0.3.0\nassay_id: Parse-Evercode-WT-v2-single-index\nname: Single-Indexed Parse Evercode WT v2\ndoi: https://support.parsebiosciences.com/hc/en-us/article_attachments/24507636161940\ndate: 2024-02-20\ndescription: Parse Evercode WT v2 using a single Illumina multiplex index\nmodalities: rna\nlib_struct: https://igvf.github.io/seqspec/\nsequence_protocol: Illumina NextSeq 2000\nsequence_kit: Illumina NextSeq 2000 P3 XLEAP-SBS Reagent kit\nlibrary_protocol: Parse Evercode WT v2\nlibrary_kit: Parse Evercode WT v2 Single Index"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_single.html#step-2-alter-the-sequence-spec",
    "href": "pages/seqspec/technologies/Parse/parse_v2_single.html#step-2-alter-the-sequence-spec",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Single-Indexed Library",
    "section": "Step 2: Alter the Sequence Spec",
    "text": "Step 2: Alter the Sequence Spec\n\nUpdate the Read Objects\nParse recommends that you sequence Read 1 to at least 66 nucleotides, but you may have decided to sequence to up to 150 nucleotides. If that is the case, you can modify the length of Read 1 by changing the min_len and max_len to reflect the read length. For example, if you want to sequence 150 nucleotides, upate the read as follows:\n\n- !Read\n  read_id: read_1\n  name: Read 1\n  modality: rna\n  primer_id: truseq_read1\n  min_len: 150\n  max_len: 150\n  strand: pos\n  files:\n  - !File\n    ...\n\nYou may have also decided not to sequence the sample indices if you did not sequence multiple samples at once. In this case, you should remove the Index 1 and Index 2 read objects.\n\n\nAdd in the file objects\nNext, for each read, you should fill out a file object referencing the corresponding file.\nA file object in seqspec if formatted as follows:\n\n- !File\n      file_id: \n      filename:\n      filetype: \n      filesize: \n      url: \n      urltype: \n      md5: \n\nfile_id, filetype, filename — identifies and describes the file\nfilesize - the size of the file\nurl - (optional) the link to the file\nurltype - (optional) the type of url\nmd5 - (optional) MD5 fingerprint of data\n\n\n\n\n\n\nNoteWhat does md5 stand for?\n\n\n\nAn MD5 checksum is a short, unique string of letters and numbers generated from a file’s contents. It acts like a digital fingerprint — if even a single byte in the file changes, the checksum will be different.\nIncluding the MD5 value for your files allows others to verify data integrity after download or transfer, ensuring that the file has not been corrupted or altered.\nTo generate an MD5 checksum string, type into your command-line:\nmd5sum &lt;filename&gt; # For Linux\n\nmd5 &lt;filename&gt; # For macOS\n\nGet-FileHash -Algorithm MD5 -Path &lt;filename&gt; # For Windows"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_single.html#step-3-alter-the-library-spec",
    "href": "pages/seqspec/technologies/Parse/parse_v2_single.html#step-3-alter-the-library-spec",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Single-Indexed Library",
    "section": "Step 3: Alter the Library Spec",
    "text": "Step 3: Alter the Library Spec\nYou can modify the min_len and max_len values for the cDNA region to reflect your desired read length. For example, if you want to sequence 150 nucleotides of cDNA, update the region as follows:\n\n    - !Region\n        parent_id: rna\n        region_id: cdna\n        region_type: cdna\n        name: cDNA\n        sequence_type: random\n        sequence: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        min_len: 150\n        max_len: 150\n        onlist: null\n        regions: null\n\nAdjust the number of X characters in the sequence field to match the new length, and then adjust the full library sequence and length accordingly.\n\nlibrary_spec:\n- !Region\n  region_id: rna\n  region_type: named\n  name: rna\n  sequence_type: joined\n  sequence: AATGATACGGCGACCACCGAGATCTACACNNNNNNNNTCTTTCCCTACACGACGCTCTTCCGATCTXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXNNNNNNNNCCACAGTCTCAAGCACGTGGATNNNNNNNNAGTCGTACGCCGATGCGAAACATCGGCCACNNNNNNNNNNNNNNNNNNAGATCGGAAGAGCACACGTCTGAACTCCAGTCACNNNNNNNNATCTCGTATGCCGTCTTCTGCTTG\n  min_len: 368\n  max_len: 368\n  onlist: null\n  parent_id: null\n  regions:\n  - !Region\n  ...\n\n\nNow you have a complete seqspec file for your data. All that’s left is to save it with your dataset!"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v3.html",
    "href": "pages/seqspec/technologies/10x/10x_v3.html",
    "title": "seqspec Tutorial: 10X Chromium Next GEM Single Cell 3’ v3.1 Dual-Indexed Library",
    "section": "",
    "text": "To download a template seqspec, click here, and then follow this tutorial to tailor the seqspec to your dataset."
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v3.html#summmary-of-library-structure-and-sequencing",
    "href": "pages/seqspec/technologies/10x/10x_v3.html#summmary-of-library-structure-and-sequencing",
    "title": "seqspec Tutorial: 10X Chromium Next GEM Single Cell 3’ v3.1 Dual-Indexed Library",
    "section": "Summmary of Library Structure and Sequencing",
    "text": "Summmary of Library Structure and Sequencing\n\n\n\n\n\nAbove is a diagram of the 10X Chromium Single Cell 3’ Dual-Indexed v3 Library. The library is structured as follows from 5’ to 3’:\n\n\n\nSequence\nType\nLength\n\n\n\n\nIllumina P5 Primer\nFixed\n29 bp\n\n\nIllumina i5 Index\nFixed\n10 bp\n\n\nTruseq Read 1\nFixed\n29 bp\n\n\nCell Barcode\nRandom\n16 bp\n\n\nUMI\nRandom\n12 bp\n\n\ncDNA\nRandom\nVariable\n\n\nTruseq Read 2\nFixed\n34 bp\n\n\nIllumina i7 Index\nFixed\n10 bp\n\n\nIllumina P7 Primer\nFixed\n24 bp\n\n\n\n\n10x Genomics recommends that the libary be sequenced in the following way:\n\n\n\nName\nPrimer\nCycles\nStrand\nCoverage\n\n\n\n\nRead 1\nTruseq Read 1\n28\npositive\nCell Barcode + UMI\n\n\nIndex 1\nTruseq Read 2\n10\npositive\nIllumina i7 Index\n\n\nIndex 2\nTruseq Read 1\n10\nnegative\nIllumina i5 Index\n\n\nRead 2\nTruseq Read 2\n90\nnegative\ncDNA"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v3.html#step-1-fill-in-the-assay-info",
    "href": "pages/seqspec/technologies/10x/10x_v3.html#step-1-fill-in-the-assay-info",
    "title": "seqspec Tutorial: 10X Chromium Next GEM Single Cell 3’ v3.1 Dual-Indexed Library",
    "section": "Step 1: Fill in the Assay Info",
    "text": "Step 1: Fill in the Assay Info\nFill in the missing assay info according to the technology you used to sequence your library and the library kit that you used. For instance, if you used the Chromium Next GEM Chip G Single Cell Kit, 48 rxns PN-1000120 reagent kit to construct your library and then the Illumina NextSeq 2000 sequencer with the P3 reagent kit, your assay info section would look like:\n!Assay\nseqspec_version: 0.3.0\nassay_id: 10x-RNA-v3.1-dual-index\nname: Dual Indexed 10x Genomics Chromium Single Cell 3' v3.1 \ndoi: https://cdn.10xgenomics.com/image/upload/v1722285481/support-documents/CG000315_ChromiumNextGEMSingleCell3__GeneExpression_v3.1_DualIndex__RevF.pdf\ndate: 2024-07-29\ndescription: 10x Genomics Chromium Single Cell 3' v3.1 using dual Illumina multiplex index\nmodalities: rna\nlib_struct: https://teichlab.github.io/scg_lib_structs/methods_html/10xChromium3.html\nsequence_protocol: Illumina NextSeq 2000\nsequence_kit: Illumina NextSeq 2000 P3 XLEAP-SBS Reagent kit\nlibrary_protocol: Single-Cell RNA Sequencing Assay (OBI:0002631)\nlibrary_kit: Chromium Next GEM Chip G Single Cell Kit, 48 rxns PN-1000120"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v3.html#step-2-alter-the-sequence-spec",
    "href": "pages/seqspec/technologies/10x/10x_v3.html#step-2-alter-the-sequence-spec",
    "title": "seqspec Tutorial: 10X Chromium Next GEM Single Cell 3’ v3.1 Dual-Indexed Library",
    "section": "Step 2: Alter the Sequence Spec",
    "text": "Step 2: Alter the Sequence Spec\n\nUpdate the Read Objects\n10x Genomics recommends that you sequence Read 2 to at least 90 nucleotides, but you may have decided to sequence to up to 150 nucleotides. If that is the case, you can modify the length of Read 2 by changing the min_len and max_len to reflect the read length. For example, if you want to sequence 150 nucleotides, update the read as follows:\n- !Read\n  read_id: read_2\n  name: Read 2\n  modality: rna\n  primer_id: truseq_read2\n  min_len: 150\n  max_len: 150\n  strand: neg\n  files:\n  - !File\n    ...\nYou may have also decided not to sequence the sample indices if you did not sequence multiple samples at once. In this case, you should remove the Index 1 and Index 2 read objects.\n\n\nAdd in the file objects\nNext, for each read, you should fill out a file object referencing the corresponding file.\nA file object in seqspec if formatted as follows:\n- !File\n      file_id: \n      filename:\n      filetype: \n      filesize: \n      url: \n      urltype: \n      md5: \nfile_id, filetype, filename — identifies and describes the file\nfilesize - the size of the file\nurl - (optional) the link to the file\nurltype - (optional) the type of url\nmd5 - (optional) MD5 fingerprint of data\n\n\n\n\n\n\nNoteWhat does md5 stand for?\n\n\n\nAn MD5 checksum is a short, unique string of letters and numbers generated from a file’s contents. It acts like a digital fingerprint — if even a single byte in the file changes, the checksum will be different.\nIncluding the MD5 value for your files allows others to verify data integrity after download or transfer, ensuring that the file has not been corrupted or altered.\nTo generate an MD5 checksum string, type into your command-line:\nmd5sum &lt;filename&gt; # For Linux\n\nmd5 &lt;filename&gt; # For macOS\n\nGet-FileHash -Algorithm MD5 -Path &lt;filename&gt; # For Windows"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v3.html#step-3-alter-the-library-spec",
    "href": "pages/seqspec/technologies/10x/10x_v3.html#step-3-alter-the-library-spec",
    "title": "seqspec Tutorial: 10X Chromium Next GEM Single Cell 3’ v3.1 Dual-Indexed Library",
    "section": "Step 3: Alter the Library Spec",
    "text": "Step 3: Alter the Library Spec\nYou can modify the min_len and max_len values for the cDNA region to reflect your desired read length. For example, if you want to sequence 150 nucleotides of cDNA, update the region as follows:\n    - !Region\n        parent_id: rna\n        region_id: cdna\n        region_type: cdna\n        name: cDNA\n        sequence_type: random\n        sequence: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        min_len: 150\n        max_len: 150\n        onlist: null\n        regions: null\nAdjust the number of X characters in the sequence field to match the new length, and then adjust the full library sequence and length accordingly.\nlibrary_spec: \n- !Region\n  region_id: rna\n  region_type: named\n  name: rna\n  sequence_type: joined\n  sequence: AATGATACGGCGACCACCGAGATCTACACNNNNNNNNNNTCTTTCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXGATCGGAAGAGCACACGTCTGAACTCCAGTCACNNNNNNNNNNATCTCGTATGCCGTCTTCTGCTTG\n  min_len: 314\n  max_len: 314\n  onlist: null\n  parent_id: null\n  regions:\n  - !Region\n    ...\n\nNow you have a complete seqspec file for your data. All that’s left is to save it with your dataset!"
  },
  {
    "objectID": "pages/installation/conda.html",
    "href": "pages/installation/conda.html",
    "title": "Creating a Conda Environment",
    "section": "",
    "text": "Conda is an open-source package and environment management system that simplifies software installation and version control. It ensures that scientific computing workflows are reproducible, portable, and isolated — ideal for complex bioinformatics pipelines.\n\nInstall and manage software (including Python and R packages) along with all their dependencies\nCreate isolated environments so different projects can use different versions of the same software without conflicts.\nReproduce analyses easily since an environment file (environment.yml) can capture the exact software versions used in your workflow.\n\nThis makes conda especially useful for computational biology and single-cell RNA-seq analysis, where multiple specialized tools (e.g., kb-python, scanpy, seqspec) must work together seamlessly.\n\n\n\n\n\n\nNoteInstalling Conda\n\n\n\nTo install Conda, follow the Miniconda installation instructions.\nMiniconda is the lightweight version of Anaconda and is recommended for most users."
  },
  {
    "objectID": "pages/installation/conda.html#why-should-i-use-conda",
    "href": "pages/installation/conda.html#why-should-i-use-conda",
    "title": "Creating a Conda Environment",
    "section": "",
    "text": "Conda is an open-source package and environment management system that simplifies software installation and version control. It ensures that scientific computing workflows are reproducible, portable, and isolated — ideal for complex bioinformatics pipelines.\n\nInstall and manage software (including Python and R packages) along with all their dependencies\nCreate isolated environments so different projects can use different versions of the same software without conflicts.\nReproduce analyses easily since an environment file (environment.yml) can capture the exact software versions used in your workflow.\n\nThis makes conda especially useful for computational biology and single-cell RNA-seq analysis, where multiple specialized tools (e.g., kb-python, scanpy, seqspec) must work together seamlessly.\n\n\n\n\n\n\nNoteInstalling Conda\n\n\n\nTo install Conda, follow the Miniconda installation instructions.\nMiniconda is the lightweight version of Anaconda and is recommended for most users."
  },
  {
    "objectID": "pages/installation/conda.html#create-a-new-environment",
    "href": "pages/installation/conda.html#create-a-new-environment",
    "title": "Creating a Conda Environment",
    "section": "Create a New Environment",
    "text": "Create a New Environment\nThere are many ways to organize your Conda environments, depending on your workflow and software dependencies. As a general rule, create a dedicated Conda environment for each project to prevent version conflicts and ensure reproducibility.\nTo create a conda environment (with Python), type into the command line:\nconda create --name myenv\nand replace myenv with your desired environment name. Then, to activate your environment:\nconda activate myenv\nNow, anything you run in this shell will use only the packages that you have explicitly installed. To deactivate your environment, run:\nconda deactivate\n\n\n\n\n\n\nNotePython or R?\n\n\n\nTo create your environment with a specific version of Python (e.g version 3.12), type into your command line:\nconda create --name myenv python=3.12\nIf your project uses R instead you can create an environment as follows (here using version 4.2):\nconda create --name myenv r-essentials r-base=4.2"
  },
  {
    "objectID": "pages/installation/conda.html#installing-a-package-with-conda",
    "href": "pages/installation/conda.html#installing-a-package-with-conda",
    "title": "Creating a Conda Environment",
    "section": "Installing a Package with Conda",
    "text": "Installing a Package with Conda\nTo install a package with Conda, you usually have to specify a channel — an online repository where packages are stored and distributed. Conda channels are online repositories where packages are stored and distributed. When you install a package with conda, it looks for that package in one or more channels (for example, defaults, conda-forge, or bioconda).\n\ndefaults — maintained by Anaconda; includes general-purpose packages.\nconda-forge - a community-maintained channel providing up-to-date, cross-platform scientific software.\nbioconda — specializes in bioinformatics software and builds on conda-forge for its dependencies.\n\nWhen installing bioinformatics tools like kb-python or seqspec, it’s best practice to specify both conda-forge and bioconda as channels to ensure compatibility:\nconda install -c conda-forge -c bioconda kb-python"
  },
  {
    "objectID": "pages/installation/conda.html#recreate-an-old-environment",
    "href": "pages/installation/conda.html#recreate-an-old-environment",
    "title": "Creating a Conda Environment",
    "section": "Recreate an Old Environment",
    "text": "Recreate an Old Environment\nYou can save your conda environment in a YAML file with:\nconda env export --from-history &gt; environment.yml\nThe --from-history flag saves only the packages you explicitly installed, keeping the file concise. If you prefer to include all dependencies and versions, omit this flag.\nTo recreate the same environment elsewhere:\nconda env create -f environment.yml --name myenv"
  },
  {
    "objectID": "pages/installation/fastqc_install.html",
    "href": "pages/installation/fastqc_install.html",
    "title": "Installing FastQC",
    "section": "",
    "text": "FastQC is written in Java, so before installing it, you must ensure that Java is properly installed on your machine (ideally the latest version). To check, open your terminal and run:\njava -version\nIf Java has been properly installed you should get an output like:\njava version \"1.8.0_471\"\nJava(TM) SE Runtime Environment (build 1.8.0_471-b09)\nJava HotSpot(TM) 64-Bit Server VM (build 25.471-b09, mixed mode)"
  },
  {
    "objectID": "pages/installation/fastqc_install.html#ensure-java-is-properly-installed",
    "href": "pages/installation/fastqc_install.html#ensure-java-is-properly-installed",
    "title": "Installing FastQC",
    "section": "",
    "text": "FastQC is written in Java, so before installing it, you must ensure that Java is properly installed on your machine (ideally the latest version). To check, open your terminal and run:\njava -version\nIf Java has been properly installed you should get an output like:\njava version \"1.8.0_471\"\nJava(TM) SE Runtime Environment (build 1.8.0_471-b09)\nJava HotSpot(TM) 64-Bit Server VM (build 25.471-b09, mixed mode)"
  },
  {
    "objectID": "pages/installation/fastqc_install.html#install-fastqc",
    "href": "pages/installation/fastqc_install.html#install-fastqc",
    "title": "Installing FastQC",
    "section": "Install FastQC",
    "text": "Install FastQC\n\n\n\n\n\n\nNoteInstalling by OS\n\n\n\nThis tutorial uses general installation commands that apply across operating systems. For detailed, OS-specific instructions, see the official FastQC installation guide.\n\n\nThe entire FastQC program is distributed as a standalone zip archive. Download the latest version (v0.12.1 at the time of writing) from the official FastQC website.\nOnce downloaded, navigate to the directory containing the file and run:\n# Unzip file\ntar -xf /path/to/fastqc_v0.12.1.zip\n\n# Make fastqc executable\nchmod 755 /path/to/FastQC/fastqc\n\n# (Optional) Make a symbolic link so you can run FASTQC from anywhere\nsudo ln -s /path/to/FastQC/fastqc /usr/local/bin/fastqc\n\n\n\n\n\n\nImportantSudo Access\n\n\n\nIf you don’t have sudo privileges on your machine, ask your system administrator to install FastQC for you. Otherwise, you’ll need to call the FastQC executable directly using a relative path each time:\n./path/to/FastQC/fastqc ...\nrather than simply:\nfastqc ..."
  },
  {
    "objectID": "pages/technologies/bulk.html",
    "href": "pages/technologies/bulk.html",
    "title": "Analyzing Your Bulk RNA-seq Assay Data",
    "section": "",
    "text": "If you do not already have FastQC installed, please refer to Installing FastQC.\nYou can launch the FastQC graphical interface by simply running:\nfastqc\nAlternatively, you can run FastQC directly from the command line:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nwhere you list all the FASTQ files you wish to analyze after the fastqc command, and outdir is the directory in which you would like the output files to be stored.\nFor a detailed tutorial on using FastQC and interpreting the output reports, see our page on Quality Control of Raw Sequencing Data with FastQC."
  },
  {
    "objectID": "pages/technologies/bulk.html#step-0-get-started-with-seqspec",
    "href": "pages/technologies/bulk.html#step-0-get-started-with-seqspec",
    "title": "Analyzing Your Bulk RNA-seq Assay Data",
    "section": "",
    "text": "To ensure that all data generated at Caltech are standardized and reproducible, we require that every dataset include a seqspec file. A seqspec file provides a machine-readable description of your experiment, specifying the assay information, library structure, and read structure in a YAML-formatted file."
  },
  {
    "objectID": "pages/technologies/bulk.html#step-1-pseudoalign-your-library-reads-with-kb-python",
    "href": "pages/technologies/bulk.html#step-1-pseudoalign-your-library-reads-with-kb-python",
    "title": "Analyzing Your Bulk RNA-seq Assay Data",
    "section": "",
    "text": "Aligning bulk RNA-seq reads is relatively straightforward. After generating a reference index with kb ref, pseudoalign your reads to the reference with kb count.\n\n\nHere we provide a brief example of how to pseudoalign a bulk library to a reference using kb count. For a more in-depth tutorial, see our page on kb-python.\nTo psuedoalign your bulk data, run:\nkb count --h5ad -x BULK -g t2g.txt -i index.idx \\\n  R1.fastq.gz R2.fastq.gz\nwhere index.idx and t2g.txt are files generated by kb ref, and R1.fastq.gz and R2.fastq.gz are your raw paired-end reads. The --h5ad argument outputs an .h5ad file suitable for downstream processing with Scanpy.\nIf you performed a bulk nucleus RNA-seq experiment, generate your index with kb ref using the NAC workflow. Then, run kb count with the standard workflow as shown above."
  },
  {
    "objectID": "pages/technologies/bulk.html#step-2-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "href": "pages/technologies/bulk.html#step-2-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "title": "Analyzing Your Bulk RNA-seq Assay Data",
    "section": "Step 2: Assess the Quality of Your Sequencing Data with FastQC",
    "text": "Step 2: Assess the Quality of Your Sequencing Data with FastQC\nIf you do not already have FastQC installed, please refer to Installing FastQC.\nYou can launch the FastQC graphical interface by simply running:\nfastqc\nAlternatively, you can run FastQC directly from the command line:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nwhere you list all the FASTQ files you wish to analyze after the fastqc command, and outdir is the directory in which you would like the output files to be stored.\nFor a detailed tutorial on using FastQC and interpreting the output reports, see our page on Quality Control of Raw Sequencing Data with FastQC."
  },
  {
    "objectID": "pages/technologies/bulk.html#step-3-process-and-analyze-your-data",
    "href": "pages/technologies/bulk.html#step-3-process-and-analyze-your-data",
    "title": "Analyzing Your Bulk RNA-seq Assay Data",
    "section": "Step 3: Process and Analyze Your Data",
    "text": "Step 3: Process and Analyze Your Data\nNow you are ready to process your data! To continue with downstream analysis, follow our example bulk rna-seq pipeline."
  },
  {
    "objectID": "pages/technologies/parse.html",
    "href": "pages/technologies/parse.html",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "",
    "text": "To ensure that all data generated at Caltech are standardized and reproducible, we require that every dataset include a seqspec file. A seqspec file provides a machine-readable description of your experiment, specifying the assay information, library structure, and read structure in a YAML-formatted file."
  },
  {
    "objectID": "pages/technologies/parse.html#step-0-get-started-with-seqspec",
    "href": "pages/technologies/parse.html#step-0-get-started-with-seqspec",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "",
    "text": "To ensure that all data generated at Caltech are standardized and reproducible, we require that every dataset include a seqspec file. A seqspec file provides a machine-readable description of your experiment, specifying the assay information, library structure, and read structure in a YAML-formatted file."
  },
  {
    "objectID": "pages/technologies/parse.html#step-1-pseudoalign-your-library-reads-with-kb-python",
    "href": "pages/technologies/parse.html#step-1-pseudoalign-your-library-reads-with-kb-python",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "Step 1: Pseudoalign Your Library Reads with kb-python",
    "text": "Step 1: Pseudoalign Your Library Reads with kb-python\nAligning reads from a Parse Evercode WT library requires additional consideration compared to other scRNA-seq assays. Unlike most platforms, which use primers targeting a specific region of each mRNA molecule, Parse Evercode WT employs two types of primers — poly(T) primers that bind to the 3′ poly(A) tail and random oligo primers that bind to random internal regions of the transcript. Each primer type has its own corresponding set of cell-specific barcodes. As a result, each cell is represented by two barcodes rather than one, as in assays such as 10x Genomics.\nTo generate a unified count matrix from the raw reads, the counts associated with these two barcodes must be collapsed so that each cell is represented once. In kb-python, this is accomplished by replacing all random oligo barcodes with their corresponding poly(T) barcodes using the -r argument.\n\n\n\n\n\n\nImportant\n\n\n\nWhen using -r to specify a replacement list in kb count, two count matrices will be produced: one with the original barcodes (stored in the output file counts_unfiltered) and one with the replacement barcodes (stored in the output file counts_unfiltered_modified)."
  },
  {
    "objectID": "pages/technologies/parse.html#an-example-v2",
    "href": "pages/technologies/parse.html#an-example-v2",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "An Example (v2)",
    "text": "An Example (v2)\nHere we provide a brief example of how to pseudoalign a Parse Evercode WT v2 library to a reference using kb count. For a more in-depth tutorial, see our page on kb-python.\nTo psuedoalign your Parse data, run:\nkb count --h5ad -x SPLIT-SEQ -r v2_replace.txt \\\n  -g t2g.txt -i index.idx R1.fastq.gz R2.fastq.gz\nwhere index.idx and t2g.txt are files generated by kb-ref, and R1.fastq.gz and R2.fastq.gz are your raw paired-end reads. The replace.txt file specifies the barcode substitution. The --h5ad argument generates an h5ad formatted file for downstream processing with Scanpy.\nIf you performed a single-nucleus RNA-seq experiment, instead use the NAC workflow to pseaudoalign both nascent and mature reads:\nkb count --h5ad -r v2_replace.txt --workflow=nac -x 10XV3 -g t2g.txt -i index.idx \\\n  -c1 cdna.txt -c2 nascent.txt R1.fastq.gz R2.fastq.gz\nHere, cdna.txt and nascent.text are files generated by kb ref -workflow=nac.\nYou can download the file v2_replace.txt for the v2 chemistry here."
  },
  {
    "objectID": "pages/technologies/parse.html#an-example-v3",
    "href": "pages/technologies/parse.html#an-example-v3",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "An Example (v3)",
    "text": "An Example (v3)\nCurrently, kb-python does not include a built-in specification for Parse Evercode WT v3. Instead, we must provide the assay information manually by specifying:\n\nv3_onlist.txt — a list of valid cell barcodes\na technology string defining barcode positions within the read: -x \"1,10,18,1,30,38,1,50,58:1,0,10:0,0,0\"\nv3_replace.txt - mapping between poly(T) and random oligo barcodes\nthe orientation of the first read (R1) with relative to your library specification: --strand=forward\nthe read parity (paired if both paired-end reads contain the biological sequence and single otherwise): --paired=single\n\nThen, run:\nkb count --h5ad --strand=forward --parity=single \\\n  -r v3_replace.txt -w v3_onlist.txt \\\n  -x \"1,10,18,1,30,38,1,50,58:1,0,10:0,0,0 \\\n  -g t2g.txt -i index.idx R1.fastq.gz R2.fastq.gz"
  },
  {
    "objectID": "pages/technologies/parse.html#step-2-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "href": "pages/technologies/parse.html#step-2-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "Step 2: Assess the Quality of Your Sequencing Data with FastQC",
    "text": "Step 2: Assess the Quality of Your Sequencing Data with FastQC\nIf you do not already have FastQC installled, please refer to Installing FastQC.\nYou can launch the FastQC graphical interface by simply running:\nfastqc\nAlternatively, you can run FastQC directly from the command line:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nwhere you list all the FASTQ files you wish to analyze after the fastqc command, and outdir is the directory in which you would like the output files to be stored.\nFor a detailed tutorial on using FastQC and interpreting the output reports, see our page on Quality Control of Raw Sequencing Data with FastQC."
  },
  {
    "objectID": "pages/technologies/parse.html#step-3-process-and-analyze-your-data",
    "href": "pages/technologies/parse.html#step-3-process-and-analyze-your-data",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "Step 3: Process and Analyze Your Data",
    "text": "Step 3: Process and Analyze Your Data\nNow you are ready to process your data! To continue with downstream analysis, follow our example scRNA-seq pipeline."
  },
  {
    "objectID": "pages/technologies/seqfish.html",
    "href": "pages/technologies/seqfish.html",
    "title": "seqFISH",
    "section": "",
    "text": "seqFISH is a powerful imaging-based single-cell transcriptomics technique that enables the spatial mapping of gene expression directly within tissue samples. Unlike droplet-based methods such as 10x Chromium or Parse Evercode, seqFISH preserves spatial context—allowing you to see not only which genes are active, but where they’re expressed within the cellular architecture.\n\n\n\nFigure: seqFish Mouse Embryo Map\n\n\nSee an example seqFISH analysis workflow here."
  },
  {
    "objectID": "pages/technologies/10x.html",
    "href": "pages/technologies/10x.html",
    "title": "Analyzing Your 10x Chromium Single Cell 3’ scRNA-seq Assay Data",
    "section": "",
    "text": "To ensure that all data generated at Caltech are standardized and reproducible, we require that every dataset include a seqspec file. A seqspec file provides a machine-readable description of your experiment, specifying the assay information, library structure, and read structure in a YAML-formatted file."
  },
  {
    "objectID": "pages/technologies/10x.html#step-0-get-started-with-seqspec",
    "href": "pages/technologies/10x.html#step-0-get-started-with-seqspec",
    "title": "Analyzing Your 10x Chromium Single Cell 3’ scRNA-seq Assay Data",
    "section": "",
    "text": "To ensure that all data generated at Caltech are standardized and reproducible, we require that every dataset include a seqspec file. A seqspec file provides a machine-readable description of your experiment, specifying the assay information, library structure, and read structure in a YAML-formatted file."
  },
  {
    "objectID": "pages/technologies/10x.html#step-1-pseudoalign-your-library-reads-with-kb-python",
    "href": "pages/technologies/10x.html#step-1-pseudoalign-your-library-reads-with-kb-python",
    "title": "Analyzing Your 10x Chromium Single Cell 3’ scRNA-seq Assay Data",
    "section": "Step 1: Pseudoalign Your Library Reads with kb-python",
    "text": "Step 1: Pseudoalign Your Library Reads with kb-python\nAligning reads from a 10x Genomics Library is relatively straightforward. After generating a reference index with kb ref, pseudoalign your reads to the reference with kb count.\n\nAn Example\nHere we provide a brief example of how to pseudoalign a 10x library to a reference using kb count. For a more in-depth tutorial, see our page on kb-python.\nTo psuedoalign your scRNA-seq 10x data, run:\nkb count --h5ad -x 10XV3 -g t2g.txt -i index.idx \\\n  R1.fastq.gz R2.fastq.gz\nwhere index.idx and t2g.txt are files generated by kb ref, and R1.fastq.gz and R2.fastq.gz are your raw paired-end reads. The --h5ad argument outputs an .h5ad file suitable for downstream processing with Scanpy.\nIf you performed a single-nucleus RNA-seq experiment, instead use the NAC workflow to pseudoalign both nascent and mature reads:\nkb count --h5ad -x 10XV3 --workflow=nac -g t2g.txt -i index.idx  \\ \n  -c1 cdna.txt -c2 nascent.txt R1.fastq.gz R2.fastq.gz\nHere, cdna.txt and nascent.text are files generated by kb ref --workflow=nac.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is specifically for the 10x Chromium Single Cell Next Gem v3 chemistry. For the v4 chemistry, replace the technology string -x 10XV3 with -x 10XV4."
  },
  {
    "objectID": "pages/technologies/10x.html#step-2-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "href": "pages/technologies/10x.html#step-2-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "title": "Analyzing Your 10x Chromium Single Cell 3’ scRNA-seq Assay Data",
    "section": "Step 2: Assess the Quality of Your Sequencing Data with FastQC",
    "text": "Step 2: Assess the Quality of Your Sequencing Data with FastQC\nIf you do not already have FastQC installled, please refer to Installing FastQC.\nYou can launch the FastQC graphical interface by simply running:\nfastqc\nAlternatively, you can run FastQC directly from the command line:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nwhere you list all the FASTQ files you wish to analyze after the fastqc command, and outdir is the directory in which you would like the output files to be stored.\nFor a detailed tutorial on using FastQC and interpreting the output reports, see our page on Quality Control of Raw Sequencing Data with FastQC."
  },
  {
    "objectID": "pages/technologies/10x.html#step-3-process-and-analyze-your-data",
    "href": "pages/technologies/10x.html#step-3-process-and-analyze-your-data",
    "title": "Analyzing Your 10x Chromium Single Cell 3’ scRNA-seq Assay Data",
    "section": "Step 3: Process and Analyze Your Data",
    "text": "Step 3: Process and Analyze Your Data\nNow you are ready to process your data! To continue with downstream analysis, follow our example scRNA-seq pipeline."
  },
  {
    "objectID": "pages/installation/R_packages.html",
    "href": "pages/installation/R_packages.html",
    "title": "Installing R Packages",
    "section": "",
    "text": "How you install R packages depends on whether you are using a conda environment. See our page on conda environments to learn how conda can help with version control and reproducibility. If you’re working within a conda environment, follow the conda installation instructions. Otherwise, you can install packages within your R script."
  },
  {
    "objectID": "pages/installation/R_packages.html#conda-installation",
    "href": "pages/installation/R_packages.html#conda-installation",
    "title": "Installing R Packages",
    "section": "Conda Installation",
    "text": "Conda Installation\n    conda install -c conda-forge r-&lt;package_name&gt;\nReplace &lt;package_name&gt; with the name of the package you wish to install."
  },
  {
    "objectID": "pages/installation/R_packages.html#r-installation",
    "href": "pages/installation/R_packages.html#r-installation",
    "title": "Installing R Packages",
    "section": "R Installation",
    "text": "R Installation\nIn R, you can install packages directly from within your script using the install.packages() function. For example, to install ggplot2, a popular package for data visualization, include the following line at the beginning of your script:\ninstall.packages(\"ggplot2\")"
  },
  {
    "objectID": "pages/installation/python_packages.html",
    "href": "pages/installation/python_packages.html",
    "title": "Installing Python Packages",
    "section": "",
    "text": "How you install Python packages depends on whether you are using a conda environment. See our page on conda environments to learn how conda can help with version control and reproducibility. If you’re working within a conda environment, follow the conda installation instructions. Otherwise, you can install packages using pip."
  },
  {
    "objectID": "pages/installation/python_packages.html#kb-python",
    "href": "pages/installation/python_packages.html#kb-python",
    "title": "Installing Python Packages",
    "section": "kb-python",
    "text": "kb-python\n\nConda\nconda install -c conda-forge -c bioconda kb-python\n\n\npip\npip install kb-python"
  },
  {
    "objectID": "pages/installation/python_packages.html#seqspec",
    "href": "pages/installation/python_packages.html#seqspec",
    "title": "Installing Python Packages",
    "section": "seqspec",
    "text": "seqspec\n\nConda\nconda install -c conda-forge -c bioconda seqspec\n\n\npip\npip install seqspec"
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Welcome to CBRC-SPEC Tutorials!",
    "section": "",
    "text": "This site is designed to guide Caltech students and staff through the full process of working with single-cell data—from formatting and processing to analysis—using the equipment and experimental setups commonly found at Caltech. Whether you’re just getting started or looking to refine your workflow, these tutorials provide step-by-step instructions tailored to our unique environment."
  },
  {
    "objectID": "pages/index.html#explore-our-single-cell-workflows",
    "href": "pages/index.html#explore-our-single-cell-workflows",
    "title": "Welcome to CBRC-SPEC Tutorials!",
    "section": "Explore Our Single Cell Workflows",
    "text": "Explore Our Single Cell Workflows\n\nClick a technology below to access step-by-step tutorials tailored to each single-cell assay.\n\n\nSingle-Cell RNA Sequencing Tutorials\n\n\n\n\n10x Genomics Chromium Single Cell 3’\n\n\n\n\n\n\n\nParse Biosciences Evercode WT\n\n\n\n\n\nSpatial Transcriptomics Tutorials\n\n\n\n\nseqFISH"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v4.html",
    "href": "pages/seqspec/technologies/10x/10x_v4.html",
    "title": "seqspec Tutorial: 10X Chromium GEM-X Single Cell 3’ v4 Dual-Indexed Library",
    "section": "",
    "text": "To download a template seqspec, click here, and then follow this tutorial to tailor the seqspec to your dataset."
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v4.html#summmary-of-library-structure-and-sequencing",
    "href": "pages/seqspec/technologies/10x/10x_v4.html#summmary-of-library-structure-and-sequencing",
    "title": "seqspec Tutorial: 10X Chromium GEM-X Single Cell 3’ v4 Dual-Indexed Library",
    "section": "Summmary of Library Structure and Sequencing",
    "text": "Summmary of Library Structure and Sequencing\n\n\n\n\n\nAbove is a diagram of the 10X Chromium Single Cell 3’ Dual-Indexed v4 Library. The library is structured as follows from 5’ to 3’:\n\n\n\nSequence\nType\nLength\n\n\n\n\nIllumina P5 Primer\nFixed\n29 bp\n\n\nIllumina i5 Index\nFixed\n10 bp\n\n\nTruseq Read 1\nFixed\n29 bp\n\n\nCell Barcode\nRandom\n16 bp\n\n\nUMI\nRandom\n12 bp\n\n\ncDNA\nRandom\nVariable\n\n\nTruseq Read 2\nFixed\n34 bp\n\n\nIllumina i7 Index\nFixed\n10 bp\n\n\nIllumina P7 Primer\nFixed\n24 bp\n\n\n\n\n10x Genomics recommends that the libary be sequenced in the following way:\n\n\n\nName\nPrimer\nCycles\nStrand\nCoverage\n\n\n\n\nRead 1\nTruseq Read 1\n28\npositive\nCell Barcode + UMI\n\n\nIndex 1\nTruseq Read 2\n10\npositive\nIllumina i7 Index\n\n\nIndex 2\nTruseq Read 1\n10\nnegative\nIllumina i5 Index\n\n\nRead 2\nTruseq Read 2\n90\nnegative\ncDNA"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v4.html#step-1-fill-in-the-assay-info",
    "href": "pages/seqspec/technologies/10x/10x_v4.html#step-1-fill-in-the-assay-info",
    "title": "seqspec Tutorial: 10X Chromium GEM-X Single Cell 3’ v4 Dual-Indexed Library",
    "section": "Step 1: Fill in the Assay Info",
    "text": "Step 1: Fill in the Assay Info\nFill in the missing assay info according to the technology you used to sequence your library. For instance, if you used the Chromium GEM-X Single Cell 3’ Kit v4, 16 rxns PN-1000691 reagent kit to construct your library and then the Illumina NextSeq 2000 sequencer with the P3 reagent kit, your assay info section would look like:\n!Assay\nseqspec_version: 0.3.0\nassay_id: 10x-RNA-v4\nname: Dual Indexed 10x Genomics Chromium Single Cell 3' v4\ndoi: https://cdn.10xgenomics.com/image/upload/v1725314293/support-documents/CG000732_ChromiumGEM-X_SingleCell3_v4_CellSurfaceProtein_UserGuide_RevB.pdf\ndate: 2025-04-18\ndescription: 10x Genomics Chromium Single Cell 3' v4 using dual Illumina multiplex index\nmodalities: rna\nlib_struct: https://teichlab.github.io/scg_lib_structs/methods_html/10xChromium3.html\nsequence_protocol: Illumina NextSeq 2000\nsequence_kit: Illumina NextSeq 2000 P3 XLEAP-SBS Reagent kit\nlibrary_protocol: Single-Cell RNA Sequencing Assay (OBI:0002631)\nlibrary_kit: Chromium GEM-X Single Cell 3' Kit v4, 16 rxns PN-1000691"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v4.html#step-2-alter-the-sequence-spec",
    "href": "pages/seqspec/technologies/10x/10x_v4.html#step-2-alter-the-sequence-spec",
    "title": "seqspec Tutorial: 10X Chromium GEM-X Single Cell 3’ v4 Dual-Indexed Library",
    "section": "Step 2: Alter the Sequence Spec",
    "text": "Step 2: Alter the Sequence Spec\n\nUpdate the Read Objects\n10x Genomics recommends that you sequence Read 2 to at least 90 nucleotides, but you may have decided to sequence to up to 150 nucleotides. If that is the case, you can modify the length of Read 2 by changing the min_len and max_len to reflect the read length. For example, if you want to sequence 150 nucleotides, update the read as follows:\n- !Read\n  read_id: read_2\n  name: Read 2\n  modality: rna\n  primer_id: truseq_read2\n  min_len: 150\n  max_len: 150\n  strand: neg\n  files:\n  - !File\n    ...\nYou may have also decided not to sequence the sample indices if you did not sequence multiple samples at once. In this case, you should remove the Index 1 and Index 2 read objects.\n\n\nAdd in the file objects\nNext, for each read, you should fill out a file object referencing the corresponding file.\nA file object in seqspec if formatted as follows:\n- !File\n      file_id: \n      filename:\n      filetype: \n      filesize: \n      url: \n      urltype: \n      md5: \nfile_id, filetype, filename — identifies and describes the file\nfilesize - the size of the file\nurl - (optional) the link to the file\nurltype - (optional) the type of url\nmd5 - (optional) MD5 fingerprint of data\n\n\n\n\n\n\nNoteWhat does md5 stand for?\n\n\n\nAn MD5 checksum is a short, unique string of letters and numbers generated from a file’s contents. It acts like a digital fingerprint — if even a single byte in the file changes, the checksum will be different.\nIncluding the MD5 value for your files allows others to verify data integrity after download or transfer, ensuring that the file has not been corrupted or altered.\nTo generate an MD5 checksum string, type into your command-line:\nmd5sum &lt;filename&gt; # For Linux\n\nmd5 &lt;filename&gt; # For macOS\n\nGet-FileHash -Algorithm MD5 -Path &lt;filename&gt; # For Windows"
  },
  {
    "objectID": "pages/seqspec/technologies/10x/10x_v4.html#step-3-alter-the-library-spec",
    "href": "pages/seqspec/technologies/10x/10x_v4.html#step-3-alter-the-library-spec",
    "title": "seqspec Tutorial: 10X Chromium GEM-X Single Cell 3’ v4 Dual-Indexed Library",
    "section": "Step 3: Alter the Library Spec",
    "text": "Step 3: Alter the Library Spec\nYou can modify the min_len and max_len values for the cDNA region to reflect your desired read length. For example, if you want to sequence 150 nucleotides of cDNA, update the region as follows:\n    - !Region\n        parent_id: rna\n        region_id: cdna\n        region_type: cdna\n        name: cDNA\n        sequence_type: random\n        sequence: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        min_len: 150\n        max_len: 150\n        onlist: null\n        regions: null\nAdjust the number of X characters in the sequence field to match the new length, and then adjust the full library sequence and length accordingly.\nlibrary_spec: \n- !Region\n  region_id: rna\n  region_type: named\n  name: rna\n  sequence_type: joined\n  sequence: AATGATACGGCGACCACCGAGATCTACACNNNNNNNNNNTCTTTCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXGATCGGAAGAGCACACGTCTGAACTCCAGTCACNNNNNNNNNNATCTCGTATGCCGTCTTCTGCTTG\n  min_len: 314\n  max_len: 314\n  onlist: null\n  parent_id: null\n  regions:\n  - !Region\n    ...\n\nNow you have a complete seqspec file for your data. All that’s left is to save it with your dataset!"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_dual.html",
    "href": "pages/seqspec/technologies/Parse/parse_v2_dual.html",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Dual-Indexed Library",
    "section": "",
    "text": "Follow one of the following links to download a template seqspec according to the Parse WT kit that you used to generate your sequencing library:\n\nParse Evercode WT Mini\nParse Evercode WT\nParse Evercode WT Mega\n\nThen, follow this tutorial to tailor the seqspec to your dataset."
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_dual.html#summmary-of-library-structure-and-sequencing",
    "href": "pages/seqspec/technologies/Parse/parse_v2_dual.html#summmary-of-library-structure-and-sequencing",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Dual-Indexed Library",
    "section": "Summmary of Library Structure and Sequencing",
    "text": "Summmary of Library Structure and Sequencing\n\n\n\n\n\nAbove is a diagram of the Parse Evercode WT v2 Dual-Indexed Library. The library is structured as follows from 5’ to 3’:\n\n\n\nSequence\nType\nLength\n\n\n\n\nIllumina P5 Primer\nFixed\n29 bp\n\n\nIllumina i5 Index\nFixed\n8 bp\n\n\nTruseq Read 1\nFixed\n29 bp\n\n\ncDNA\nRandom\nVariable\n\n\nRound 1 Barcode\nOnList\n8 bp\n\n\nLinker\nFixed\n22 bp\n\n\nRound 2 Barcode\nOnList\n8 bp\n\n\nLinker\nFixed\n30 bp\n\n\nRound 3 Barcode\nOnList\n8 bp\n\n\nUMI\nRandom\n10 bp\n\n\nTruseq Read 2\nFixed\n34 bp\n\n\nIllumina i7 Index\nFixed\n10 bp\n\n\nIllumina P7 Primer\nFixed\n24 bp\n\n\n\n\nParse Biosciences recommends that the libary be sequenced in the following way:\n\n\n\n\n\n\n\n\n\n\nName\nPrimer\nCycles\nStrand\nCoverage\n\n\n\n\nRead 1\nTruseq Read 1\n66\npositive\ncDNA\n\n\nIndex 1\nTruseq Read 2\n8\npositive\nIllumina i7 Index\n\n\nIndex 2\nTruseq Read 1\n8\nnegative\nIllumina i5 Index\n\n\nRead 2\nTruseq Read 2\n86\nnegative\nUMI + Barcodes + Linkers"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_dual.html#step-1-fill-in-the-assay-info",
    "href": "pages/seqspec/technologies/Parse/parse_v2_dual.html#step-1-fill-in-the-assay-info",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Dual-Indexed Library",
    "section": "Step 1: Fill in the Assay Info",
    "text": "Step 1: Fill in the Assay Info\nFill in the missing assay info according to the technology you used to sequence your library. For instance, if you used the Illumina NextSeq 2000 sequencer with the P3 reagent kit, your assay info section would look like:\n!Assay\nseqspec_version: 0.3.0\nassay_id: Parse-Evercode-WT-v2-dual-index\nname: Dual-Indexed Parse Evercode WT v2\ndoi: https://support.parsebiosciences.com/hc/en-us/article_attachments/24507636161940\ndate: 2024-02-20\ndescription: Parse Evercode WT v2 using dual Illumina multiplex index\nmodalities: rna\nlib_struct: https://igvf.github.io/seqspec/\nsequence_protocol: Illumina NextSeq 2000\nsequence_kit: Illumina NextSeq 2000 P3 XLEAP-SBS Reagent kit\nlibrary_protocol: Parse Evercode WT v2\nlibrary_kit: Parse Evercode WT v2 Dual Index"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_dual.html#step-2-alter-the-sequence-spec",
    "href": "pages/seqspec/technologies/Parse/parse_v2_dual.html#step-2-alter-the-sequence-spec",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Dual-Indexed Library",
    "section": "Step 2: Alter the Sequence Spec",
    "text": "Step 2: Alter the Sequence Spec\n\nUpdate the Read Objects\nParse recommends that you sequence Read 1 to at least 66 nucleotides, but you may have decided to sequence to up to 150 nucleotides. If that is the case, you can modify the length of Read 1 by changing the min_len and max_len to reflect the read length. For example, if you want to sequence 150 nucleotides, upate the read as follows:\n- !Read\n  read_id: read_1\n  name: Read 1\n  modality: rna\n  primer_id: truseq_read1\n  min_len: 150\n  max_len: 150\n  strand: pos\n  files:\n  - !File\n    ...\nYou may have also decided not to sequence the sample indices if you did not sequence multiple samples at once. In this case, you should remove the Index 1 and Index 2 read objects.\n\n\nAdd in the file objects\nNext, for each read, you should fill out a file object referencing the corresponding file.\nA file object in seqspec if formatted as follows:\n- !File\n      file_id: \n      filename:\n      filetype: \n      filesize: \n      url: \n      urltype: \n      md5: \nfile_id, filetype, filename — identifies and describes the file\nfilesize - the size of the file\nurl - (optional) the link to the file\nurltype - (optional) the type of url\nmd5 - (optional) MD5 fingerprint of data\n\n\n\n\n\n\nNoteWhat does md5 stand for?\n\n\n\nAn MD5 checksum is a short, unique string of letters and numbers generated from a file’s contents. It acts like a digital fingerprint — if even a single byte in the file changes, the checksum will be different.\nIncluding the MD5 value for your files allows others to verify data integrity after download or transfer, ensuring that the file has not been corrupted or altered.\nTo generate an MD5 checksum string, type into your command-line:\nmd5sum &lt;filename&gt; # For Linux\n\nmd5 &lt;filename&gt; # For macOS\n\nGet-FileHash -Algorithm MD5 -Path &lt;filename&gt; # For Windows"
  },
  {
    "objectID": "pages/seqspec/technologies/Parse/parse_v2_dual.html#step-3-alter-the-library-spec",
    "href": "pages/seqspec/technologies/Parse/parse_v2_dual.html#step-3-alter-the-library-spec",
    "title": "seqspec Tutorial: Parse Evercode WT v2 Dual-Indexed Library",
    "section": "Step 3: Alter the Library Spec",
    "text": "Step 3: Alter the Library Spec\nYou can modify the min_len and max_len values for the cDNA region to reflect your desired read length. For example, if you want to sequence 150 nucleotides of cDNA, update the region as follows:\n    - !Region\n        parent_id: rna\n        region_id: cdna\n        region_type: cdna\n        name: cDNA\n        sequence_type: random\n        sequence: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        min_len: 150\n        max_len: 150\n        onlist: null\n        regions: null\nAdjust the number of X characters in the sequence field to match the new length, and then adjust the full library sequence and length accordingly.\nlibrary_spec:\n- !Region\n  region_id: rna\n  region_type: named\n  name: rna\n  sequence_type: joined\n  sequence: AATGATACGGCGACCACCGAGATCTACACNNNNNNNNTCTTTCCCTACACGACGCTCTTCCGATCTXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXNNNNNNNNCCACAGTCTCAAGCACGTGGATNNNNNNNNAGTCGTACGCCGATGCGAAACATCGGCCACNNNNNNNNNNNNNNNNNNAGATCGGAAGAGCACACGTCTGAACTCCAGTCACNNNNNNNNATCTCGTATGCCGTCTTCTGCTTG\n  min_len: 368\n  max_len: 368\n  onlist: null\n  parent_id: null\n  regions:\n  - !Region\n  ...\n\nNow you have a complete seqspec file for your data. All that’s left is to save it with your dataset!"
  },
  {
    "objectID": "pages/seqspec/seqspec.html",
    "href": "pages/seqspec/seqspec.html",
    "title": "What is seqspec?",
    "section": "",
    "text": "seqspec is a simple, standardized file format to describe how a genomics library was prepared and sequenced. Different genomics assays require unique processing steps. Including a seqspec file with your data makes it easy for others (and future you!) to reanalyze your dataset. Here we will give a brief description of the seqspec format. For a more in-depth description of seqspec and the seqspec file format, refer to the GitHub page and the paper."
  },
  {
    "objectID": "pages/seqspec/seqspec.html#understanding-the-seqspec-file-format",
    "href": "pages/seqspec/seqspec.html#understanding-the-seqspec-file-format",
    "title": "What is seqspec?",
    "section": "Understanding the seqspec File Format",
    "text": "Understanding the seqspec File Format\nA seqspec file is a YAML-formatted document with three main sections:\n\nAssay Info – metadata describing the assay and protocols used\n\nLibrary Structure – layout and sequence of regions in the library\n\nRead Structure – how the sequencing reads correspond to library regions\n\nEach section depends on the specific library preparation and sequencing technology used in your assay.\n\n\n\n\n\n\nNoteWhy YAML?\n\n\n\nYAML (short for “YAML Ain’t Markup Language”) is a lightweight, human-readable format that is easy to edit and interpret, yet structured enough for automated parsing by software. This balance makes it ideal for encoding experimental metadata that must be both transparent to researchers and machine-accessible for reproducible analysis."
  },
  {
    "objectID": "pages/seqspec/seqspec.html#section-1-assay-info",
    "href": "pages/seqspec/seqspec.html#section-1-assay-info",
    "title": "What is seqspec?",
    "section": "Section 1: Assay Info",
    "text": "Section 1: Assay Info\nThe first section describes the overall experiment — what assay was used, which kit, and how it was sequenced.\n\nBasic Format\n!Assay\nseqspec_version: 0.3.0\nassay_id: \nname: \ndoi: \ndate: \ndescription:\nmodalities: rna\nlib_struct: \nsequence_protocol: \nsequence_kit: \nlibrary_protocol: \nlibrary_kit:\nseqspec_version — version of the seqspec format used (this tutorial will use version 0.3.0)\nassay_id, name, doi, date, description — metadata describing the assay\nlibrary_protocol, library_kit — kit and reagents used for library prep\nsequence_protocol, sequence_kit — sequencing instrument and reagents used\n\n\nExample\nSuppose you used the Parse Biosciences Evercode WT Mega v2.0.1 dual index kit to generate an scRNA-seq library, sequenced on Illumina NovaSeq X. The corresponding seqspec file might look like so:\n!Assay\nseqspec_version: 0.3.0\nassay_id: Evercode-WT-mega-v2-dual-index\nname: Parse Evercode Mega WT v2 using dual Illumina multiplex index\ndoi: https://www.protocols.io/view/evercode-wt-mega-v2-2-1-8epv5xxrng1b/v1?step=21\ndate: 08 November 2023\ndescription: split-pool ligation-based transcriptome sequencing\nmodalities: rna\nlib_struct: https://igvf.github.io/seqspec/\nlibrary_protocol: single-nucleus RNA sequencing assay (OBI:0003109)\nlibrary_kit: Evercode WT Mega v2.0.1 dual index\nsequence_protocol: Illumina NovaSeq X (EFO:0022840)\nsequence_kit: NovaSeq X Series 25B Reagent Kit"
  },
  {
    "objectID": "pages/seqspec/seqspec.html#section-2-library-structure",
    "href": "pages/seqspec/seqspec.html#section-2-library-structure",
    "title": "What is seqspec?",
    "section": "Section 2: Library Structure",
    "text": "Section 2: Library Structure\nThe library structure section lists all regions in the sequencing library (from 5’ to 3’), such as primers, linkers, UMIs, and barcodes.\nEach region is defined with the following template:\n- !Region\n  parent_id: \n  region_id: \n  region_type:\n  name: \n  sequence_type: \n  sequence: \n  min_len: \n  max_len: \n  onlist: \n  regions: \nregion_id, region_type, name — identifies and describes the region\nparent_id — refers to the parent region (if it exists)\nsequence, sequence_type — specifies the nucleotide sequence and its type (e.g., fixed, onlist, variable)\nmin_len, max_len — defines expected sequence lengths\nonlist — references a file or list of valid sequences (e.g., known cell barcodes)\n\nNested Regions\nParent regions can contain multiple child regions, forming a hierarchical structure. Here’s a minimal example with one parent and two child regions:\nlibrary_spec:\n- !Region\n  parent_id: null\n  region_id: parent\n  ...\n  regions:\n  - !Region\n    parent_id: parent\n    region_id: region_1\n    ...\n    regions: null\n  - !Region\n    parent_id: parent\n    region_id: region_2\n    ...\n    regions: null\n\n\nExamples\nBelow are two example regions you might find in a seqspec file.\n\nExample 1: Illumina P5 primer\n- !Region\n  parent_id: null\n  region_id: P5\n  region_type: primer\n  name: P5\n  sequence_type: fixed\n  sequence: AATGATACGGCGACCACCGAGATCTACAC\n  min_len: 29\n  max_len: 29\n  onlist: null\n  regions: null\n\n\nExample 2: Parse Biosciences Evercode WT cell barcode\n- !Region\n  parent_id: null\n  region_id: BC\n  region_type: barcode\n  name: cell barcode\n  sequence_type: onlist\n  sequence: NNNNNNNNNNNNNNNN\n  min_len: 16\n  max_len: 16\n  onlist: !Onlist\n    location: local\n    filename: onlist.txt\n  regions: null\n\n\n\n\n\n\nNoteWhat is an Onlist?\n\n\n\nIn Parse Evercode libraries, barcode regions are validated against a fixed list of known barcodes called the “onlist”."
  },
  {
    "objectID": "pages/seqspec/seqspec.html#section-3-read-structure",
    "href": "pages/seqspec/seqspec.html#section-3-read-structure",
    "title": "What is seqspec?",
    "section": "Section 3: Read Structure",
    "text": "Section 3: Read Structure\nThe read structure section links your FASTQ files to specific sequencing reads.\nEach read entry looks like this:\n- !Read\n  read_id: \n  name: \n  modality: \n  primer_id: \n  min_len: \n  max_len: \n  strand: \n  files:\nread_id, name — identifies the read object\nprimer_id — links to the corresponding primer region\nstrand — direction of sequencing relative to the library layout\nmin_len, max_len — expected read length\nfiles — references the FASTQ files containing this read\n\nExample\nsequence_spec:\n- !Read\n  read_id: read_1\n  name: Read 1\n  modality: rna\n  primer_id: truseq_read1\n  min_len: 28\n  max_len: 28\n  strand: pos\n  files:\n  - !File\n    file_id: read_1\n    filename: read_1.fastq.gz\n    filetype: fastq.gz\n    filesize: 10 GB\n    url: https://example.org/read_1.fastq.gz\n    urltype: https\n    md5: d1410f93b53357d4f95edb0ba0d73c07\n\n\nStep 3: Validate Your seqspec file with seqspec check\nYou can use the seqspec check command from the seqspec package to verify your seqspec file is correctly formatted.\nseqspec check seqspec.yaml\nIf your file is valid, the command will complete without errors; otherwise, it will print a description of any issues found in the specification.\n\n\n\n\n\n\nNoteInstall seqspec\n\n\n\nIn order to install seqspec, follow our installation instructions here."
  },
  {
    "objectID": "pages/packages/fastqc.html",
    "href": "pages/packages/fastqc.html",
    "title": "Quality Control of Raw Sequencing Data with FastQC",
    "section": "",
    "text": "FastQC is a quality control tool for high-throughput sequencing data. It provides a quick, visual summary of your raw FASTQ files, helping you identify potential issues such as low-quality reads, adapter contamination, or biased base composition. FastQC generates easy-to-read HTML reports that summarize key quality metrics like per-base quality scores, GC content, and sequence duplication levels. Running FastQC before alignment or downstream analysis ensures your sequencing data are high-quality and suitable for accurate biological interpretation.\nHere we provide a brief tutorial for analyzing your FASTQ files with FastQC. For a more in-depth explanation of FastQC please refer to the website and GitHub page."
  },
  {
    "objectID": "pages/packages/fastqc.html#run-fastqc",
    "href": "pages/packages/fastqc.html#run-fastqc",
    "title": "Quality Control of Raw Sequencing Data with FastQC",
    "section": "Run FastQC",
    "text": "Run FastQC\nFastQC conveniently has a built-in GUI in which to run your analysis. To open the GUI simply type into your command-line:\nfastqc\nand follow the displayed instructions.\nIf you would prefer to analyze your data through the command-line, simply specify a list of files following the fastqc command:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nand replace outdir with the path to the directory you wish to have your output stored."
  },
  {
    "objectID": "pages/packages/fastqc.html#a-brief-tutorial-on-fastq-files",
    "href": "pages/packages/fastqc.html#a-brief-tutorial-on-fastq-files",
    "title": "Quality Control of Raw Sequencing Data with FastQC",
    "section": "A Brief Tutorial on FASTQ Files",
    "text": "A Brief Tutorial on FASTQ Files\nTo understand the FastQC output, we must first understand what a FASTQ file is and how it is generated.\nFASTQ files are the standard format for storing raw sequencing reads from next-generation sequencing instruments. Each read in a FASTQ file is represented by four lines:\n@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n\nLine 1 – Read identifier (begins with @)\nLine 2 – Nucleotide sequence\nLine 3 – Separator line (usually just +)\nLine 4 – Quality scores for each base (encoded as ASCII characters)\n\n\nWhat Are Quality Scores?\nEach entry in a FASTQ file contains not only the DNA or RNA sequence itself, but also a quality score for every base. These scores reflect the sequencing machine’s confidence in each base call — that is, how likely it is that a given nucleotide (A, T, G, or C) was correctly identified.\nFASTQ quality scores are typically encoded using the Phred scale, a logarithmic transformation of the error probability:\n\\[Q = -10\\times \\log_{10}(P)\\]\nwhere \\(P\\) is the probability that a base call is incorrect. For example, a Phred score of 20 means there’s a 1% chance the base is wrong, while a score of 30 corresponds to a 0.1% error rate.\nThese scores are stored as ASCII characters in the FASTQ file, allowing both sequence and quality information to be represented compactly.\nDuring quality control, tools like FastQC visualize these scores across all reads, helping identify low-quality regions that may need trimming or filtering before alignment and downstream analysis."
  },
  {
    "objectID": "pages/packages/fastqc.html#understanding-fastqc-plots",
    "href": "pages/packages/fastqc.html#understanding-fastqc-plots",
    "title": "Quality Control of Raw Sequencing Data with FastQC",
    "section": "Understanding FASTQC Plots",
    "text": "Understanding FASTQC Plots\nBelow are the plots that FastQC generates, each providing diagnostic insights into your sequencing data quality.\n\n\n\n1. Per-Base Sequence Quality Over All Reads\n\nThis plot shows the distribution of quality scores for each base position across all reads. Typically, quality is highest near the 5′ end and drops toward the 3′ end of reads.\n\n\n\nA Good FastQC Report\n\n\nA sharp decline in quality may indicate sequencing problems or the need for trimming low-quality bases before alignment.\n\n\n\nA Bad FastQC Report\n\n\n\n\n\n2. Per-Tile Sequence Quality Over All Reads\n\nThis graph visualizes quality variation across different tiles of the sequencing flow cell.\nEach tile corresponds to a physical area on the sequencer’s imaging surface.\nSystematic differences between tiles may point to instrument issues, uneven illumination, or problems during image analysis.\n\n\n\nA Bad FastQC Report\n\n\n\n\n\n3. Quality Score Distribution Over All Sequences\n\nThis plot summarizes the overall distribution of mean quality scores per read.\nIdeally, most reads should have consistently high average scores.\n\n\n\nA Good FastQC Report\n\n\nA broad or bimodal distribution suggests variable read quality, possibly due to uneven sequencing performance or mixed-quality samples.\n\n\n\nA Bad FastQC Report\n\n\n\n\n\n4. Sequence Content Across All Bases\n\nThis plot shows the proportion of each nucleotide (A, T, G, and C) at every base position.\nIn random libraries, the lines should be roughly parallel.\n\n\n\nA Good FastQC Report\n\n\nDeviations early in the read may indicate biased priming, adapter contamination, or non-random base composition typical of certain assays (e.g., single-cell barcoding regions).\n\n\n\n5. Per Sequence GC Content\n\nThis graph compares the GC content distribution of all reads to a theoretical normal distribution.\n\n\n\nA Good FastQC Report\n\n\nA shifted or multi-peaked curve may reflect contamination from other organisms, amplification bias, or targeted sequencing regions with atypical GC content.\n\n\n\n6. Per Base N Content\n\nThis plot shows the percentage of ambiguous bases (“N”) called at each position in the read.\n\n\n\nA Good FastQC Report\n\n\nHigh or position-specific N content indicates uncertainty in base calling and may suggest low signal intensity or sequencing chemistry problems.\n\n\n\nA Bad FastQC Report\n\n\n\n\n\n7. Sequence Length Distribution\n\nThis plot displays the distribution of read lengths in the dataset.\nFor most sequencing platforms, a single consistent read length is expected.\n\n\n\nA Good FastQC Report\n\n\nVariable lengths may indicate trimming, read merging, or incomplete read cycles.\n\n\n\n8. Sequence Duplication Levels\n\nThis plot shows how many sequences are duplicated and how often.\n\n\n\nA Good FastQC Report\n\n\nHigh duplication levels can indicate PCR amplification bias or low library complexity.\n\n\n\nA Bad FastQC Report\n\n\nHowever, in RNA-seq or scRNA-seq data, some duplication is expected due to highly expressed genes.\n\n\n\n9. Overrepresented Sequences\n\nThis section lists any sequences that occur more frequently than expected by chance.\nOverrepresentation may arise from adapter contamination, primer dimers, or abundant biological sequences such as rRNA.\n\n\n\nA Bad FastQC Report\n\n\nIdentifying and removing these sequences improves downstream analysis accuracy.\n\n\n\n10. Adapter Content\n\nThis plot quantifies how much adapter sequence remains untrimmed at each read position.\n\n\n\nA Good FastQC Report\n\n\nIncreasing adapter content toward the 3′ end of reads is a common sign that reads are longer than the actual insert fragment and require adapter trimming before alignment."
  },
  {
    "objectID": "pages/packages/kb_python.html",
    "href": "pages/packages/kb_python.html",
    "title": "Pseudoalignment of scRNA-seq Reads with kb-python",
    "section": "",
    "text": "Note\n\n\n\nThis tutorial provides a brief introduction to kb-python. For a more in-depth guide, see the official documentation and reference the papers for kb-python, bustools, and kallisto.\nkb-python is a lightweight, command-line toolkit for processing single-cell RNA sequencing (scRNA-seq) data. It provides an efficient and transparent way to convert raw sequencing reads into gene count matrices ready for downstream analysis.\nThe tool integrates two powerful components:\nTogether, these tools enable a streamlined and modular workflow that supports flexible experimental designs and efficient large-scale processing.\nA typical kb-python workflow includes:"
  },
  {
    "objectID": "pages/packages/kb_python.html#workflows",
    "href": "pages/packages/kb_python.html#workflows",
    "title": "Pseudoalignment of scRNA-seq Reads with kb-python",
    "section": "Workflows",
    "text": "Workflows\nkb-python provides three workflows tailored to different experimental setups and analysis goals. The two most commonly used are:\n\nNAC workflow – quantifies nascent (unspliced) transcripts. This workflow is typically used for single-nucleus RNA-seq experiments, where a substantial portion of captured mRNA remains unspliced.\nStandard workflow – quantifies mature (fully spliced) transcripts. This workflow is suitable when you are interested only in processed mRNA."
  },
  {
    "objectID": "pages/packages/kb_python.html#step-1-generate-a-reference-index",
    "href": "pages/packages/kb_python.html#step-1-generate-a-reference-index",
    "title": "Pseudoalignment of scRNA-seq Reads with kb-python",
    "section": "Step 1: Generate a Reference Index",
    "text": "Step 1: Generate a Reference Index\nThe first step of any RNA-seq analysis is aligning your reads to a reference genome. To do this, kb-python requires a reference index representing the set of target transcripts to which your reads will be mapped.\n\nUsing a Default Index\nkb-python conveniently has a precompiled set of indices for the following species:\n\nhuman\nmouse\ndog\nmonkey\nzebrafish\n\nFor the standard workflow, you can load the human index as follows:\nkb ref -d human -i index.idx -g t2g.txt\nwhich will create the files index.idx and t2g.txt.\nand for the nascent workflow,\nkb ref --workflow=nac -d human -i index.idx \\ \n    -g t2g.txt -c1 cdna.txt -c2 nascent.txt\nwhich will additionally generate the files cdna.txt and nascent.txt.\n\n\n\n\n\n\nNote\n\n\n\nSee how the precompiled indices were generated here.\n\n\n\n\nGenerating a Custom Index\nIn order to make a custom index, you must specify a genome FASTA reference file and a GTF annotation file from which to extract the transcript sequences. You can obtain these files from ENSEMBL. We recommend using the primary assembly FASTA file (ends .dna.primary_assembly.fa.gz).\nThen, to generate an index for the standard workflow, run:\nkb ref  -i index.idx -g t2g.txt -f1 cdna.fasta genome.fasta genome.gtf\nand for the nac workflow, run:\nkb ref --workflow=nac -i index.idx -g t2g.txt -c1 cdna.txt -c2 nascent.txt \\\n    -f1 cdna.fasta -f2 nascent.fasta genome.fasta genome.gtf"
  },
  {
    "objectID": "pages/packages/kb_python.html#step-2-align-to-reference-genome",
    "href": "pages/packages/kb_python.html#step-2-align-to-reference-genome",
    "title": "Pseudoalignment of scRNA-seq Reads with kb-python",
    "section": "Step 2: Align to Reference Genome",
    "text": "Step 2: Align to Reference Genome\nThe next step is to generate a count matrix by pseudoaligning the reads to your reference index using the kb count command. For example, suppose you have two paired-end FASTQ files (read1.fastq and read2.fastq) from a 10x Chromium NextGen v3 library.\nFor the standard workflow, you can perform pseudoalignment with:\nkb count -i index_file.idx -g t2g_file.txt -x 10XV3 \\\n    -o output_dir read1.fastq read2.fastq\nand for the nascent workflow,\nkb count --workflow=nac -i index_file.idx -g t2g_file.txt -c1 cdna.txt \\\n    -c2 nascent.txt -x 10XV3 -o output_dir read1.fastq read2.fastq\nwhere output_dir is the folder where you wish the results of kb_count to be stored.\n\n\n\n\n\n\nNoteWhat is a FASTQ file?\n\n\n\nFASTQ files are the standard format for storing raw sequencing reads from next-generation sequencing instruments. Each read in a FASTQ file is represented by four lines:\n@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n\nLine 1 – Read identifier (begins with @)\nLine 2 – Nucleotide sequence\nLine 3 – Separator line (usually just +)\nLine 4 – Quality scores for each base (encoded as ASCII characters)\n\nkb count accepts both raw and gzipped FASTQ files. If your reads are stored in another file format, you will have to convert to FASTQ before pseudoalignment with kb-python.\n\n\nChange -x 10XV3 according to the specific technology you used to generate your library.\n\n10X Chromium GEM-X V4: -x 10XV4\nParse Evercode WT v2: -x SPLITSEQ\n\nTo see a complete list of technologies and their corresponding technology strings, type kb --list into your terminal.\n\nBatch Pseudoaligment\nIn most scRNA-seq experiments, you will have multiple samples, each with its own pair of FASTQ files. Instead of running kb count separately on every sample, kb-python allows you to process many samples in a single command using the --batch-barcodes flag.\nTo perform a batch pseudoalignment, run:\nkb count ... --batch-barcodes batch.txt\nfilling in the command with the parameters specific to your experiment. The batch.txt file must be a space-separated file, with one row per sample. Each row should contain:\nsample_name read1.fastq.gz read2.fastq.gz\n— that is, the name of the sample followed by the paths to its Read 1 and Read 2 FASTQ files.\nkb-python will automatically process each sample independently and prepend a unique, artificially generated sample-specific barcode to the beginning of every cell barcode. This ensures that cell barcodes from different samples remain distinguishable in the combined output without altering the underlying cellular barcodes themselves.\nA list of these sample-specific barcode prefixes is saved in counts_unfiltered/cells_x_genes.barcodes.prefix.txt, which maps each cell barcode to its corresponding sample barcode.\n\n\nParse Evercode WT v3: Using a Custom Specification\nCurrently, kb-python does not include a built-in specification for the Parse Evercode WT v3 chemistry. To pseudoalign Parse v3 data, you must manually specify the assay configuration by providing:\n\nv3_onlist.txt — a list of valid cell barcodes\na technology string defining barcode positions within the read: -x \"1,10,18,1,30,38,1,50,58:1,0,10:0,0,0\"\nthe orientation of the first read (R1) with relative to your library specification: --strand=forward\nthe read parity (paired if both paired-end reads contain the biological sequence and single otherwise): --paired=single\n\nThen, run:\nkb count --h5ad --strand=forward --parity=single \\\n  -w v3_onlist.txt -x \"1,10,18,1,30,38,1,50,58:1,0,10:0,0,0 \\\n  -g t2g.txt -i index.idx R1.fastq.gz R2.fastq.gz\n\n\nParse Evercode WT: Substituting Barcodes with the -r Argument\nThe Parse Evercode WT chemistry is unique in that each cell has two cell-specific barcodes rather than one. Since kb-python expects a single barcode per cell, you must collapse barcode pairs before pseudoalignment. This is done using the -r argument in kb count, for example:\nkb count -r &lt;replace_file&gt; ...\nwhere &lt;replace_file&gt; is a TSV file with two columns:\n\nColumn 1: barcodes to be replaced\nColumn 2: corresponding replacement barcodes\n\nYou can download:\n\nv2_replace.txt for v2 chemistry here\nv3_replace.txt for v3 chemistry here\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using -r to specify a replacement list in kb count, two count matrices will be produced: one with the original barcodes (stored in the output file counts_unfiltered) and one with the replacement barcodes (stored in the output file counts_unfiltered_modified).\n\n\n\n\nValidating Pseudoalignment with the run_info.json file\nWhen you run kallisto through kb-python, it generates a file called run_info.json. This file summarizes essential metrics from the pseudoalignment step and helps you verify that your reads were successfully mapped to the reference transcriptome. Although it is a small file, it contains several important fields that give a quick indication of alignment quality and potential issues with your input data, reference files, or library chemistry.\n\nKey Metrics to Evaluate\n\nn_processed\nThis value reports the total number of reads (or read pairs) that kallisto attempted to pseudoalign. It should match your expectation based on the number of reads in your FASTQ files. A much smaller number than expected usually indicates truncated FASTQ files, incorrect file paths, corrupted input, or an early pipeline failure.\np_pseudoaligned\nThis is the fraction of reads that successfully pseudoaligned (n_pseudoaligned / n_processed). It is the most important single metric. Typical expectations:\n\nBulk RNA-seq: often &gt;80–90%\nscRNA-seq (10x): usually 60–90%\nMultiome or non-standard assays: more variable\n\nVery low values (e.g., &lt;30%) almost always indicate a mismatch between your reference and your reads rather than a biological.\np_unique\nThis value reports the fraction of reads that pseudoaligned uniquely to a single transcript rather than mapping to multiple possible targets. For most scRNA-seq datasets, a value above 20% is expected, though this depends heavily on the organism, assay, and read length; in some cases, values as low as 10% can still be normal.\nA low p_unique can occur when:\n\nmany transcripts share highly similar sequences (e.g., paralogs, pseudogenes),\nthe reference is incomplete or poorly annotated,\nreads are too short or low-quality,\nadapters or barcodes were not properly trimmed,\na high percentage of the reads originate from ribosomal RNA.\n\nWhile p_pseudoaligned tells you how many reads mapped at all, p_unique tells you how confidently kallisto could assign those reads.\nRuntime information\nFields like start_time, end_time, and sometimes call help you confirm that kallisto completed successfully and show exactly what command was executed."
  },
  {
    "objectID": "pages/packages/kb_python.html#next-steps",
    "href": "pages/packages/kb_python.html#next-steps",
    "title": "Pseudoalignment of scRNA-seq Reads with kb-python",
    "section": "Next Steps",
    "text": "Next Steps\nAfter generating your count matrix, you can import it into analysis tools such as Scanpy, Seurat, or Bioconductor for downstream processing, clustering, and visualization."
  },
  {
    "objectID": "notebooks/bulk_pipeline.html",
    "href": "notebooks/bulk_pipeline.html",
    "title": "\nIntroductory Bulk RNA-seq Analysis`\n",
    "section": "",
    "text": "This notebook provides a step-by-step guide to performing basic bulk RNA-seq analysis using kb-python for quantification and edgeR for downstream analysis. By the end of this tutorial, you will have learned how to process raw sequencing data, perform quality control, normalize the data, and identify differentially expressed genes."
  },
  {
    "objectID": "notebooks/bulk_pipeline.html#install-kb-python",
    "href": "notebooks/bulk_pipeline.html#install-kb-python",
    "title": "\nIntroductory Bulk RNA-seq Analysis`\n",
    "section": "Install kb-python",
    "text": "Install kb-python\n\n!pip install kb_python"
  },
  {
    "objectID": "notebooks/bulk_pipeline.html#downloads",
    "href": "notebooks/bulk_pipeline.html#downloads",
    "title": "\nIntroductory Bulk RNA-seq Analysis`\n",
    "section": "Downloads",
    "text": "Downloads\n\nDatasets\nThis notebook will analyze data from the papers: Differential analysis of gene regulation at transcript resolution with RNA-seq by Cole Trapnell, David G Henderickson, Martin Savageau, Loyal Goff, John L Rinn and Lior Pachter, Nature Biotechnology 31, 46–53 (2013).\nThe control (C) and knockdown (KD) fibroblast samples are subsetted to 1 million reads each.\n\nControl (n=3)\n\n!wget -q https://github.com/pachterlab/data/releases/download/v1/C_1_R1.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/C_1_R2.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/C_2_R1.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/C_2_R2.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/C_3_R1.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/C_3_R2.fastq.gz\n\n\n\nKnockdown (n=3)\n\n!wget -q https://github.com/pachterlab/data/releases/download/v1/KD_1_R1.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/KD_1_R2.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/KD_2_R1.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/KD_2_R2.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/KD_3_R1.fastq.gz\n!wget -q https://github.com/pachterlab/data/releases/download/v1/KD_3_R2.fastq.gz\n\n\n\n\nkallisto human index\n\n!kb ref -d human -i human_index.idx -g human_t2g.txt\n\n[2025-06-19 02:40:51,007]    INFO [download] Downloading files for human (standard workflow) from https://github.com/pachterlab/kallisto-transcriptome-indices/releases/download/v1/human_index_standard.tar.xz to tmp/human_index_standard.tar.xz\n100% 138M/138M [00:01&lt;00:00, 134MB/s]\n[2025-06-19 02:40:52,095]    INFO [download] Extracting files from tmp/human_index_standard.tar.xz"
  },
  {
    "objectID": "notebooks/bulk_pipeline.html#kb-python-quantification",
    "href": "notebooks/bulk_pipeline.html#kb-python-quantification",
    "title": "\nIntroductory Bulk RNA-seq Analysis`\n",
    "section": "kb-python Quantification",
    "text": "kb-python Quantification\n\nKallisto Quantification and Technical Uncertainty\nKallisto does not produce raw read counts the way alignment-based RNA-seq pipelines do. Instead, it uses a probabilistic model of pseudoalignment to estimate transcript abundances. For each read \\(r\\), kallisto determines the set of transcripts \\(T_r\\) it is compatible with and assigns abundance parameters \\(\\lambda_t\\) by maximizing the likelihood\n\\[ L(\\lambda \\mid \\text{reads}) = \\prod_{r=1}^{N} \\left( \\sum_{t \\in T_r} \\frac{\\lambda_t}{l_t}\\right),\\]\nwhere \\(l_t\\) is the effective length of transcript \\(t\\). Because these abundances are estimated rather than directly counted, they carry quantification uncertainty, which is especially pronounced for genes with multi-mapping reads or overlapping isoforms.\n\n\nBootstrap Variance as a Measure of Technical Dispersion\nTo measure this quantification uncertainty, kallisto performs bootstrap resampling, repeatedly re-quantifying the same sample by resampling reads with replacement. This produces bootstrap estimates \\(\\hat{\\lambda}_t^{(1)}, \\ldots, \\hat{\\lambda}_t^{(B)}\\) whose variability\n\\[ \\widehat{\\mathrm{Var}}_{\\text{boot}}(\\hat{\\lambda}_t) = \\frac{1}{B-1} \\sum_{b=1}^{B} \\left(\\hat{\\lambda}_t^{(b)} - \\overline{\\hat{\\lambda}_t}\\right)^2, \\]\nreflects technical uncertainty in the abundance estimates. Functions such as catchKallisto convert these bootstrap variances into per-gene “overdispersion” values that summarize the precision of kallisto’s quantification for each transcript in each sample.\n\n\nMap reads to index\nBelow, we quantify RNA-seq reads using kallisto (via kb count) with bootstrapping (via --bootstraps) to obtain estimates of quantification uncertainty. Here, we set the number of bootstraps to 10.\nFor the input to kb count, we supply all FASTQ files on the command-line (and the order in which the files are supplied determines the sample identities after read quantification).\n\n!kb count -x BULK -i human_index.idx -g human_t2g.txt --parity=paired --tcc --matrix-to-directories -o output_dir \\\n--bootstraps=5 -t 2 --overwrite --verbose \\\nC_1_R1.fastq.gz C_1_R2.fastq.gz C_2_R1.fastq.gz C_2_R2.fastq.gz C_3_R1.fastq.gz C_3_R2.fastq.gz \\\nKD_1_R1.fastq.gz KD_1_R2.fastq.gz KD_2_R1.fastq.gz KD_2_R2.fastq.gz KD_3_R1.fastq.gz KD_3_R2.fastq.gz\n\n[2025-06-19 02:41:28,558]   DEBUG [main] Printing verbose output\n[2025-06-19 02:41:30,761]   DEBUG [main] kallisto binary located at /usr/local/lib/python3.11/dist-packages/kb_python/bins/linux/kallisto/kallisto\n[2025-06-19 02:41:30,761]   DEBUG [main] bustools binary located at /usr/local/lib/python3.11/dist-packages/kb_python/bins/linux/bustools/bustools\n[2025-06-19 02:41:30,761]   DEBUG [main] Creating `output_dir/tmp` directory\n[2025-06-19 02:41:30,762]   DEBUG [main] Namespace(list=False, command='count', tmp=None, keep_tmp=False, verbose=True, i='human_index.idx', g='human_t2g.txt', x='BULK', o='output_dir', num=False, w=None, exact_barcodes=False, r=None, t=2, m='2G', strand=None, inleaved=False, genomebam=False, aa=False, gtf=None, chromosomes=None, workflow='standard', em=False, mm=False, tcc=True, filter=None, filter_threshold=None, c1=None, c2=None, overwrite=True, dry_run=False, batch_barcodes=False, loom=False, h5ad=False, loom_names='barcode,target_name', sum='none', cellranger=False, gene_names=False, N=None, report=False, no_inspect=False, long=False, threshold=0.8, error_rate=None, platform='ONT', kallisto='/usr/local/lib/python3.11/dist-packages/kb_python/bins/linux/kallisto/kallisto', bustools='/usr/local/lib/python3.11/dist-packages/kb_python/bins/linux/bustools/bustools', opt_off=False, k=31, no_validate=False, no_fragment=False, union=False, no_jump=False, quant_umis=False, keep_flags=False, parity='paired', fragment_l=None, fragment_s=None, bootstraps=5, matrix_to_files=False, matrix_to_directories=True, fastqs=['C_1_R1.fastq.gz', 'C_1_R2.fastq.gz', 'C_2_R1.fastq.gz', 'C_2_R2.fastq.gz', 'C_3_R1.fastq.gz', 'C_3_R2.fastq.gz', 'KD_1_R1.fastq.gz', 'KD_1_R2.fastq.gz', 'KD_2_R1.fastq.gz', 'KD_2_R2.fastq.gz', 'KD_3_R1.fastq.gz', 'KD_3_R2.fastq.gz'])\n[2025-06-19 02:41:33,929]    INFO [count] Using index human_index.idx to generate BUS file to output_dir from\n[2025-06-19 02:41:33,929]    INFO [count]         C_1_R1.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         C_1_R2.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         C_2_R1.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         C_2_R2.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         C_3_R1.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         C_3_R2.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         KD_1_R1.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         KD_1_R2.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         KD_2_R1.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         KD_2_R2.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         KD_3_R1.fastq.gz\n[2025-06-19 02:41:33,929]    INFO [count]         KD_3_R2.fastq.gz\n[2025-06-19 02:41:33,929]   DEBUG [count] kallisto bus -i human_index.idx -o output_dir -x BULK -t 2 --paired C_1_R1.fastq.gz C_1_R2.fastq.gz C_2_R1.fastq.gz C_2_R2.fastq.gz C_3_R1.fastq.gz C_3_R2.fastq.gz KD_1_R1.fastq.gz KD_1_R2.fastq.gz KD_2_R1.fastq.gz KD_2_R2.fastq.gz KD_3_R1.fastq.gz KD_3_R2.fastq.gz\n[2025-06-19 02:41:33,933]   DEBUG [count] \n[2025-06-19 02:41:50,491]   DEBUG [count] [index] k-mer length: 31\n[2025-06-19 02:41:58,110]   DEBUG [count] [index] number of targets: 227,665\n[2025-06-19 02:41:58,110]   DEBUG [count] [index] number of k-mers: 139,900,295\n[2025-06-19 02:41:58,110]   DEBUG [count] [index] number of D-list k-mers: 5,477,475\n[2025-06-19 02:41:58,210]   DEBUG [count] [quant] running in paired-end mode\n[2025-06-19 02:41:58,210]   DEBUG [count] [quant] will process pair 1: C_1_R1.fastq.gz\n[2025-06-19 02:41:58,210]   DEBUG [count] C_1_R2.fastq.gz\n[2025-06-19 02:41:58,210]   DEBUG [count] [quant] will process pair 1: C_2_R1.fastq.gz\n[2025-06-19 02:41:58,210]   DEBUG [count] C_2_R2.fastq.gz\n[2025-06-19 02:41:58,210]   DEBUG [count] [quant] will process pair 1: C_3_R1.fastq.gz\n[2025-06-19 02:41:58,210]   DEBUG [count] C_3_R2.fastq.gz\n[2025-06-19 02:41:58,210]   DEBUG [count] [quant] will process pair 1: KD_1_R1.fastq.gz\n[2025-06-19 02:41:58,210]   DEBUG [count] KD_1_R2.fastq.gz\n[2025-06-19 02:41:58,211]   DEBUG [count] [quant] will process pair 1: KD_2_R1.fastq.gz\n[2025-06-19 02:41:58,211]   DEBUG [count] KD_2_R2.fastq.gz\n[2025-06-19 02:41:58,211]   DEBUG [count] [quant] will process pair 1: KD_3_R1.fastq.gz\n[2025-06-19 02:41:58,211]   DEBUG [count] KD_3_R2.fastq.gz\n[2025-06-19 02:42:36,516]   DEBUG [count] [quant] finding pseudoalignments for all files ...\n[2025-06-19 02:43:12,851]   DEBUG [count] [progress] 1M reads processed (92.9% mapped)\n[2025-06-19 02:43:47,534]   DEBUG [count] [progress] 2M reads processed (92.9% mapped)\n[2025-06-19 02:44:23,935]   DEBUG [count] [progress] 3M reads processed (92.6% mapped)\n[2025-06-19 02:45:01,667]   DEBUG [count] [progress] 4M reads processed (92.4% mapped)\n[2025-06-19 02:45:31,676]   DEBUG [count] [progress] 5M reads processed (92.4% mapped)              done\n[2025-06-19 02:45:31,676]   DEBUG [count] [quant] processed 6,000,000 reads, 5,541,414 reads pseudoaligned\n[2025-06-19 02:45:32,177]   DEBUG [count] \n[2025-06-19 02:45:34,181]    INFO [count] Sorting BUS file output_dir/output.bus to output_dir/tmp/output.s.bus\n[2025-06-19 02:45:34,181]   DEBUG [count] bustools sort -o output_dir/tmp/output.s.bus -T output_dir/tmp -t 2 -m 2G output_dir/output.bus\n[2025-06-19 02:45:35,887]   DEBUG [count] partition time: 0.034993s\n[2025-06-19 02:45:36,995]   DEBUG [count] all fits in buffer\n[2025-06-19 02:45:38,196]   DEBUG [count] Read in 5541414 BUS records\n[2025-06-19 02:45:38,196]   DEBUG [count] reading time 0.05138s\n[2025-06-19 02:45:38,196]   DEBUG [count] sorting time 1.35408s\n[2025-06-19 02:45:38,196]   DEBUG [count] writing time 0.057799s\n[2025-06-19 02:45:38,196]    INFO [count] Inspecting BUS file output_dir/tmp/output.s.bus\n[2025-06-19 02:45:38,196]   DEBUG [count] bustools inspect -o output_dir/inspect.json output_dir/tmp/output.s.bus\n[2025-06-19 02:45:39,298]    INFO [count] Generating count matrix output_dir/counts_unfiltered/cells_x_tcc from BUS file output_dir/tmp/output.s.bus\n[2025-06-19 02:45:39,298]   DEBUG [count] bustools count -o output_dir/counts_unfiltered/cells_x_tcc -g human_t2g.txt -e output_dir/matrix.ec -t output_dir/transcripts.txt --multimapping --cm output_dir/tmp/output.s.bus\n[2025-06-19 02:45:42,276]   DEBUG [count] output_dir/counts_unfiltered/cells_x_tcc.mtx passed validation\n[2025-06-19 02:45:42,276]    INFO [count] Quantifying transcript abundances to output_dir/quant_unfiltered from mtx file output_dir/counts_unfiltered/cells_x_tcc.mtx\n[2025-06-19 02:45:42,276]   DEBUG [count] kallisto quant-tcc -o output_dir/quant_unfiltered -i human_index.idx -e output_dir/counts_unfiltered/cells_x_tcc.ec.txt -g human_t2g.txt -t 2 -f output_dir/flens.txt -b 5 --matrix-to-directories output_dir/counts_unfiltered/cells_x_tcc.mtx\n[2025-06-19 02:45:42,377]   DEBUG [count] \n[2025-06-19 02:45:58,624]   DEBUG [count] [index] k-mer length: 31\n[2025-06-19 02:46:07,976]   DEBUG [count] [index] number of targets: 227,665\n[2025-06-19 02:46:07,976]   DEBUG [count] [index] number of k-mers: 139,900,295\n[2025-06-19 02:46:07,976]   DEBUG [count] [index] number of D-list k-mers: 5,477,475\n[2025-06-19 02:46:09,979]   DEBUG [count] [index] number of equivalence classes loaded from file: 404,862\n[2025-06-19 02:46:09,979]   DEBUG [count] [index] not using the D-list k-mers\n[2025-06-19 02:46:09,979]   DEBUG [count] [tcc] Parsing transcript-compatibility counts (TCC) file as a matrix file\n[2025-06-19 02:46:09,979]   DEBUG [count] [tcc] Matrix dimensions: 6 x 404,862\n[2025-06-19 02:46:09,979]   DEBUG [count] [tcc] Bootstrapping will be performed and outputted as HDF5\n[2025-06-19 02:46:10,881]   DEBUG [count] [quant] Running EM algorithm...\n[2025-06-19 02:46:10,881]   DEBUG [count] [quant] Processing sample/cell 0\n[2025-06-19 02:46:10,881]   DEBUG [count] [quant] Processing sample/cell 1\n[2025-06-19 02:59:55,076]   DEBUG [count] [quant] Processing sample/cell 2\n[2025-06-19 02:59:55,076]   DEBUG [count] [quant] Processing sample/cell 3\n[2025-06-19 03:14:38,980]   DEBUG [count] [quant] Processing sample/cell formed and outputted as HDF5 a [quant] Processing sample/cell formed and outputted as HDF5 a 5\n[2025-06-19 03:14:38,980]   DEBUG [count] 4\n[2025-06-19 03:28:33,037]   DEBUG [count] done\n[2025-06-19 03:28:33,038]   DEBUG [count] \n[2025-06-19 03:28:36,457]   DEBUG [count] output_dir/quant_unfiltered/matrix.abundance.gene.mtx passed validation\n[2025-06-19 03:28:36,468]   DEBUG [count] output_dir/quant_unfiltered/matrix.abundance.gene.tpm.mtx passed validation\n[2025-06-19 03:28:36,495]   DEBUG [count] output_dir/quant_unfiltered/matrix.abundance.mtx passed validation\n[2025-06-19 03:28:36,521]   DEBUG [count] output_dir/quant_unfiltered/matrix.abundance.tpm.mtx passed validation\n[2025-06-19 03:28:36,523]   DEBUG [main] Removing `output_dir/tmp` directory\n\n\n\n\nNow inspect the output\nThere are several “abundance” directories that contain our quantifications. The numbers correspond to the order in which we supplied the FASTQ files to the kb count command (i.e. abundance_1, abundance_2, and abundance_3 are the control samples and abundance_4, abundance_5, and abundance_6 are the knockdown samples)\n\n!ls output_dir/quant_unfiltered\n\nabundance_1  abundance_6            matrix.abundance.tpm.mtx\nabundance_2  genes.txt              matrix.efflens.mtx\nabundance_3  matrix.abundance.gene.mtx      matrix.fld.tsv\nabundance_4  matrix.abundance.gene.tpm.mtx  transcript_lengths.txt\nabundance_5  matrix.abundance.mtx       transcripts.txt"
  },
  {
    "objectID": "notebooks/bulk_pipeline.html#differential-expression-of-transcripts",
    "href": "notebooks/bulk_pipeline.html#differential-expression-of-transcripts",
    "title": "\nIntroductory Bulk RNA-seq Analysis`\n",
    "section": "Differential expression of transcripts",
    "text": "Differential expression of transcripts\n\nSet up R environment and edgeR\n\n%load_ext rpy2.ipython\n\n\n%%R\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"edgeR\", ask=FALSE, quiet=TRUE)\nBiocManager::install(\"rhdf5\", ask=FALSE, quiet=TRUE)\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\ntrying URL 'https://cran.rstudio.com/src/contrib/BiocManager_1.30.26.tar.gz'\nContent type 'application/x-gzip' length 594489 bytes (580 KB)\n==================================================\ndownloaded 580 KB\n\n\nThe downloaded source packages are in\n    ‘/tmp/RtmplaovhL/downloaded_packages’\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://cran.rstudio.com\nBioconductor version 3.21 (BiocManager 1.30.26), R 4.5.1 (2025-06-13)\nInstalling package(s) 'BiocVersion', 'edgeR'\nalso installing the dependencies ‘statmod’, ‘limma’, ‘locfit’\n\nOld packages: 'data.table', 'evaluate'\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://cran.rstudio.com\nBioconductor version 3.21 (BiocManager 1.30.26), R 4.5.1 (2025-06-13)\nInstalling package(s) 'rhdf5'\nalso installing the dependencies ‘Rhdf5lib’, ‘rhdf5filters’"
  },
  {
    "objectID": "notebooks/bulk_pipeline.html#use-edger-for-differential-transcript-expression-analysis",
    "href": "notebooks/bulk_pipeline.html#use-edger-for-differential-transcript-expression-analysis",
    "title": "\nIntroductory Bulk RNA-seq Analysis`\n",
    "section": "Use edgeR for differential transcript expression analysis",
    "text": "Use edgeR for differential transcript expression analysis\nWe now format the data for use with edgeR via the edgeR catchKallisto function. catchKallisto reads in the kallisto output files and returns a matrix of transcript counts ($counts) and a data.frame ($annotation) of transcript information. This transcript information includes the estimated overdispersion coefficients for each transcript obtained via the kallisto bootstraps.\n\n%%R\nrequire(edgeR)\npaths &lt;- c(\"output_dir/quant_unfiltered/abundance_1\",\n           \"output_dir/quant_unfiltered/abundance_2\",\n           \"output_dir/quant_unfiltered/abundance_3\",\n           \"output_dir/quant_unfiltered/abundance_4\",\n           \"output_dir/quant_unfiltered/abundance_5\",\n           \"output_dir/quant_unfiltered/abundance_6\")\nresults &lt;- catchKallisto(paths)\n\nReading output_dir/quant_unfiltered/abundance_1, 227665 transcripts, 5 bootstraps\nReading output_dir/quant_unfiltered/abundance_2, 227665 transcripts, 5 bootstraps\nReading output_dir/quant_unfiltered/abundance_3, 227665 transcripts, 5 bootstraps\nReading output_dir/quant_unfiltered/abundance_4, 227665 transcripts, 5 bootstraps\nReading output_dir/quant_unfiltered/abundance_5, 227665 transcripts, 5 bootstraps\nReading output_dir/quant_unfiltered/abundance_6, 227665 transcripts, 5 bootstraps\n\n\nLoading required package: edgeR\nLoading required package: limma\n\n\n\nUsing Overdispersion to Weight Counts for edgeR\nWhen preparing the data for edgeR, the estimated counts are divided by these overdispersion values. This step acts as a precision weighting: transcripts whose abundances vary widely across bootstraps (high quantification uncertainty) are down-weighted, while transcripts with stable bootstrap estimates (high precision) are given greater influence.\nAfter this adjustment, edgeR estimates biological dispersion across replicates using its standard negative binomial model,\n\\[\\mathrm{Var}(X) = \\mu + \\phi \\mu^{2},\\]\nwhere \\(\\phi\\) captures true biological variability between samples.\nIn this workflow, kallisto’s bootstraps handle technical uncertainty, and edgeR models biological variability—ensuring each source of noise is represented appropriately during differential expression analysis.​\n\n%%R\n# Print the first 5 rows of the counts matrix \nprint(results$counts[1:5,,drop=F])\n# Print the number of transcripts with overdispersion estimates\nprint(length(results$annotation$Overdispersion))\n\n                  output_dir/quant_unfiltered/abundance_1\nENST00000308647.8                               18.119158\nENST00000378736.3                                0.000000\nENST00000472194.6                                3.606886\nENST00000474481.1                                1.487577\nENST00000485748.5                                1.878091\n                  output_dir/quant_unfiltered/abundance_2\nENST00000308647.8                              16.3508487\nENST00000378736.3                               0.4174387\nENST00000472194.6                               2.6497518\nENST00000474481.1                               0.0000000\nENST00000485748.5                               0.0000000\n                  output_dir/quant_unfiltered/abundance_3\nENST00000308647.8                               17.676045\nENST00000378736.3                                2.851005\nENST00000472194.6                                0.000000\nENST00000474481.1                                0.000000\nENST00000485748.5                                0.000000\n                  output_dir/quant_unfiltered/abundance_4\nENST00000308647.8                                9.134442\nENST00000378736.3                                2.485286\nENST00000472194.6                                9.018708\nENST00000474481.1                                0.000000\nENST00000485748.5                                0.000000\n                  output_dir/quant_unfiltered/abundance_5\nENST00000308647.8                               13.095408\nENST00000378736.3                                0.000000\nENST00000472194.6                                1.569073\nENST00000474481.1                                1.475829\nENST00000485748.5                                1.834814\n                  output_dir/quant_unfiltered/abundance_6\nENST00000308647.8                                5.862184\nENST00000378736.3                                1.859032\nENST00000472194.6                                2.428891\nENST00000474481.1                                0.000000\nENST00000485748.5                                0.000000\n[1] 227665\n\n\nNext, we create a DGEList object from the weighted count matrix (counts/overdispersion) and annotation data.frame. We also specify the experimental design via the group argument, indicating which samples are controls and which are knockdowns.\nThe DGEList object is composed of:\n\ncounts: a table of counts\ngroup indicators for each sample\nlibrary size (optional)\na table of feature annotations (optional)\n\n\n%%R\nsamples &lt;- c(\"Control_1\", \"Control_2\", \"Control_3\", \"Knockdown_1\", \"Knockdown_2\", \"Knockdown_3\")\ngroup &lt;- c(\"C\", \"C\", \"C\", \"K\", \"K\", \"K\") # C = control; K = knockdown\n\n\n%%R\ncolnames(results$counts) &lt;- samples\ncts.scaled &lt;- results$counts/results$annotation$Overdispersion\ndge.scaled &lt;- DGEList(counts = cts.scaled, group=group)\n\nWith our DGEList object created, we can now proceed with normalization, dispersion estimation, and differential expression testing using edgeR’s suite of functions."
  },
  {
    "objectID": "notebooks/bulk_pipeline.html#gene-filtering-and-normalization",
    "href": "notebooks/bulk_pipeline.html#gene-filtering-and-normalization",
    "title": "\nIntroductory Bulk RNA-seq Analysis`\n",
    "section": "Gene Filtering and Normalization",
    "text": "Gene Filtering and Normalization\nNext, we normalize the data with the edgeR function calcNormFactors().\nBefore normalizing the data with calcNormFactors(), it is standard practice in edgeR to remove genes that are not expressed at meaningful levels using filterByExpr(). Lowly expressed genes contribute little biological information but can add substantial noise to normalization and dispersion estimation. Many such genes have zero or near-zero counts across most samples, making their expression values unstable. Because TMM normalization (used by calcNormFactors()) compares log-fold changes between samples, these zero-dominated genes can generate extreme or undefined log-ratios that bias the normalization factors.\nBy applying filterByExpr() first, we restrict the dataset to genes that have sufficient counts in enough samples to be informative. This ensures that normalization is computed from genes with stable expression levels, improving both the accuracy of the scaling factors and the effectiveness of downstream differential expression analysis.\n\nWhat calcNormFactors() Does\nNormalization is a critical step in RNA-seq analysis because samples often differ in sequencing depth and RNA composition. Simply dividing by total library size is not always sufficient—if one sample is dominated by a handful of highly expressed genes, all other genes will appear artificially underexpressed. edgeR addresses this using TMM (Trimmed Mean of M-values) normalization, implemented in calcNormFactors().\nThe function computes a scaling factor for each sample by comparing it to a reference sample and calculating gene-wise log-fold changes (M-values). After trimming away genes with extreme log-fold changes or very high expression (which could distort the normalization), it computes a weighted average of the remaining M-values. This trimmed mean is then converted into a normalization factor:\n\\[ \\text{TMM factor} = 2^{\\text{weighted mean}(M_g)}. \\]\nThe resulting normalization factors adjust each sample’s effective library size:\n\\[N_i^{\\text{eff}} = N_i \\times \\text{norm.factor}_i,\\]\nwhere \\(N_i\\) is the original library size for sample \\(i\\) and \\(\\text{norm.factor}_i\\) is the TMM normalization factor computed by calcNormFactors(). This adjustment sllos for differences in sequencing depth and compositional bias to be corrected without altering the underlying counts themselves. These effective library sizes are then used in downstream steps such as CPM calculation, dispersion estimation, and model fitting.\nIn short, filtering ensures that normalization is guided by meaningful gene expression patterns, and calcNormFactors() then computes scaling factors that place all samples on a comparable expression scale, enabling accurate differential expression testing.\n\n%%R\nkeep &lt;- filterByExpr(dge.scaled)\ndge.scaled.filtr &lt;- dge.scaled[keep, , keep.lib.sizes = FALSE]\ndge.scaled.filtr &lt;- calcNormFactors(dge.scaled.filtr)\n\n\n\nExploring Sample Relationships with plotMDS()\nThe plotMDS() function in edgeR creates a multidimensional scaling (MDS) plot that visualizes the overall similarity between samples based on their expression profiles. This plot is an essential quality control step because it allows you to identify clustering patterns, detect batch effects, and spot potential outliers before performing differential expression analysis.\nSimilar to principal component analysis (PCA), MDS reduces the high-dimensional gene expression data into a two-dimensional representation. However, unlike PCA, which is based on total variance, plotMDS() uses leading log-fold-change distances. For each pair of samples, it computes the largest gene-wise log-fold changes and uses the average of these as a distance measure. By default, the top 500 most variable genes contribute to this distance. The MDSplot then positions samples in a 2D space such that the distances between points reflect these computed distances.\nIn the MDSplot below, dim 1 represents the greatest source of variation between samples, and dim 2 represents the second greatest. Examining these dimensions helps you verify that biological groups cluster as expected and that no technical artifacts dominate the data. If unexpected clustering or outliers appear, this can signal issues such as batch effects or mislabeled samples that should be addressed before downstream analysis.\n\n%%R\nplotMDS(dge.scaled.filtr)\n\n\n\n\n\n\n\n\n\n%%R\ndesign &lt;- model.matrix(~group-1,data = dge.scaled.filtr$samples)\ncolnames(design) &lt;- gsub('group','',colnames(design))\ndge.scaled.filtr &lt;- estimateDisp(dge.scaled.filtr,design)\nplotBCV(dge.scaled.filtr)\n\n\n\n\n\n\n\n\n\n%%R\nfit &lt;- glmQLFit(dge.scaled.filtr,design)\nplotQLDisp(fit)\n\n\n\n\n\n\n\n\n\n%%R\nqlf &lt;- glmQLFTest(fit, contrast = makeContrasts(K-C, levels = design))\ntt &lt;- topTags(qlf,n = Inf)\nis.de &lt;- decideTests(qlf)\nplotMD(qlf, status = is.de, values = c(1, -1), col = c(\"red\",\"blue\"), legend = \"topright\", cex=0.4)\n\n\n\n\n\n\n\n\n\n%%R\nplot(qlf$table$logFC, -1*log10(qlf$table$PValue), main=\"Volcano plot\", xlab=\"log2FC\", ylab=\"-log10(P-val)\")\n\n\n\n\n\n\n\n\n\n%%R\nhead(qlf$table)\n\n                          logFC   logCPM            F    PValue\nENST00000356607.9  -0.001899721 5.303589 0.0000167155 0.9968000\nENST00000328089.11  0.321175658 5.765121 1.8001385388 0.2026674\nENST00000375592.8  -0.041597613 5.700996 0.0332228001 0.8581821\nENST00000346436.11  0.453813269 4.743627 1.0396290301 0.3265557\nENST00000378512.5  -0.513499461 4.907051 1.4394727424 0.2516714\nENST00000443438.5  -0.109194711 5.451238 0.0663814381 0.8007152"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to CBRC-SPEC Tutorials!",
    "section": "",
    "text": "This site is designed to guide Caltech students and staff through the full process of working with single-cell data—from formatting and processing to analysis—using the equipment and experimental setups commonly found at Caltech. Whether you’re just getting started or looking to refine your workflow, these tutorials provide step-by-step instructions tailored to our unique environment."
  },
  {
    "objectID": "index.html#explore-our-single-cell-workflows",
    "href": "index.html#explore-our-single-cell-workflows",
    "title": "Welcome to CBRC-SPEC Tutorials!",
    "section": "Explore Our Single Cell Workflows",
    "text": "Explore Our Single Cell Workflows\n\nClick a technology below to access step-by-step tutorials tailored to each single-cell assay.\n\n\nSingle-Cell RNA Sequencing Tutorials\n\n\n\n\n10x Genomics Chromium Single Cell 3’\n\n\n\n\n\n\n\nParse Biosciences Evercode WT\n\n\n\n\n\nBulk RNA-seq Tutorials\n\n\n\n\nBulk RNA-seq\n\n\n\n\n\nSpatial Transcriptomics Tutorials\n\n\n\n\nseqFISH"
  },
  {
    "objectID": "pages/technologies/parse.html#step-1-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "href": "pages/technologies/parse.html#step-1-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "Step 1: Assess the Quality of Your Sequencing Data with FastQC",
    "text": "Step 1: Assess the Quality of Your Sequencing Data with FastQC\nIf you do not already have FastQC installled, please refer to Installing FastQC.\nYou can launch the FastQC graphical interface by simply running:\nfastqc\nAlternatively, you can run FastQC directly from the command line:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nwhere you list all the FASTQ files you wish to analyze after the fastqc command, and outdir is the directory in which you would like the output files to be stored.\nFor a detailed tutorial on using FastQC and interpreting the output reports, see our page on Quality Control of Raw Sequencing Data with FastQC."
  },
  {
    "objectID": "pages/technologies/parse.html#step-2-pseudoalign-your-library-reads-with-kb-python",
    "href": "pages/technologies/parse.html#step-2-pseudoalign-your-library-reads-with-kb-python",
    "title": "Analyzing Your Parse Biosciences Evercode WT scRNA-seq Assay Data",
    "section": "Step 2: Pseudoalign Your Library Reads with kb-python",
    "text": "Step 2: Pseudoalign Your Library Reads with kb-python\nAligning reads from a Parse Evercode WT library requires additional consideration compared to other scRNA-seq assays. Unlike most platforms, which use primers targeting a specific region of each mRNA molecule, Parse Evercode WT employs two types of primers — poly(T) primers that bind to the 3′ poly(A) tail and random oligo primers that bind to random internal regions of the transcript. Each primer type has its own corresponding set of cell-specific barcodes. As a result, each cell is represented by two barcodes rather than one, as in assays such as 10x Genomics.\nTo generate a unified count matrix from the raw reads, the counts associated with these two barcodes must be collapsed so that each cell is represented once. In kb-python, this is accomplished by replacing all random oligo barcodes with their corresponding poly(T) barcodes using the -r argument.\n\n\n\n\n\n\nImportant\n\n\n\nWhen using -r to specify a replacement list in kb count, two count matrices will be produced: one with the original barcodes (stored in the output file counts_unfiltered) and one with the replacement barcodes (stored in the output file counts_unfiltered_modified)."
  },
  {
    "objectID": "pages/technologies/10x.html#step-2-pseudoalign-your-library-reads-with-kb-python",
    "href": "pages/technologies/10x.html#step-2-pseudoalign-your-library-reads-with-kb-python",
    "title": "Analyzing Your 10x Chromium Single Cell 3’ scRNA-seq Assay Data",
    "section": "Step 2: Pseudoalign Your Library Reads with kb-python",
    "text": "Step 2: Pseudoalign Your Library Reads with kb-python\nAligning reads from a 10x Genomics Library is relatively straightforward. After generating a reference index with kb ref, pseudoalign your reads to the reference with kb count.\n\nAn Example\nHere we provide a brief example of how to pseudoalign a 10x library to a reference using kb count. For a more in-depth tutorial, see our page on kb-python.\nTo psuedoalign your scRNA-seq 10x data, run:\nkb count --h5ad -x 10XV3 -g t2g.txt -i index.idx \\\n  R1.fastq.gz R2.fastq.gz\nwhere index.idx and t2g.txt are files generated by kb ref, and R1.fastq.gz and R2.fastq.gz are your raw paired-end reads. The --h5ad argument outputs an .h5ad file suitable for downstream processing with Scanpy.\nIf you performed a single-nucleus RNA-seq experiment, instead use the NAC workflow to pseudoalign both nascent and mature reads:\nkb count --h5ad -x 10XV3 --workflow=nac -g t2g.txt -i index.idx  \\ \n  -c1 cdna.txt -c2 nascent.txt R1.fastq.gz R2.fastq.gz\nHere, cdna.txt and nascent.text are files generated by kb ref --workflow=nac.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is specifically for the 10x Chromium Single Cell Next Gem v3 chemistry. For the v4 chemistry, replace the technology string -x 10XV3 with -x 10XV4."
  },
  {
    "objectID": "pages/technologies/10x.html#step-1-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "href": "pages/technologies/10x.html#step-1-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "title": "Analyzing Your 10x Chromium Single Cell 3’ scRNA-seq Assay Data",
    "section": "Step 1: Assess the Quality of Your Sequencing Data with FastQC",
    "text": "Step 1: Assess the Quality of Your Sequencing Data with FastQC\nIf you do not already have FastQC installled, please refer to Installing FastQC.\nYou can launch the FastQC graphical interface by simply running:\nfastqc\nAlternatively, you can run FastQC directly from the command line:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nwhere you list all the FASTQ files you wish to analyze after the fastqc command, and outdir is the directory in which you would like the output files to be stored.\nFor a detailed tutorial on using FastQC and interpreting the output reports, see our page on Quality Control of Raw Sequencing Data with FastQC."
  },
  {
    "objectID": "pages/technologies/bulk.html#step-1-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "href": "pages/technologies/bulk.html#step-1-assess-the-quality-of-your-sequencing-data-with-fastqc",
    "title": "Analyzing Your Bulk RNA-seq Assay Data",
    "section": "",
    "text": "If you do not already have FastQC installed, please refer to Installing FastQC.\nYou can launch the FastQC graphical interface by simply running:\nfastqc\nAlternatively, you can run FastQC directly from the command line:\nfastqc --extract --outdir=outdir somefile.fastq someotherfile.fastq ...\nwhere you list all the FASTQ files you wish to analyze after the fastqc command, and outdir is the directory in which you would like the output files to be stored.\nFor a detailed tutorial on using FastQC and interpreting the output reports, see our page on Quality Control of Raw Sequencing Data with FastQC."
  },
  {
    "objectID": "pages/technologies/bulk.html#step-2-pseudoalign-your-library-reads-with-kb-python",
    "href": "pages/technologies/bulk.html#step-2-pseudoalign-your-library-reads-with-kb-python",
    "title": "Analyzing Your Bulk RNA-seq Assay Data",
    "section": "Step 2: Pseudoalign Your Library Reads with kb-python",
    "text": "Step 2: Pseudoalign Your Library Reads with kb-python\nAligning bulk RNA-seq reads is relatively straightforward. After generating a reference index with kb ref, pseudoalign your reads to the reference with kb count.\n\nAn Example\nHere we provide a brief example of how to pseudoalign a bulk library to a reference using kb count. For a more in-depth tutorial, see our page on kb-python.\nTo psuedoalign your bulk data, run:\nkb count --h5ad -x BULK -g t2g.txt -i index.idx \\\n  R1.fastq.gz R2.fastq.gz\nwhere index.idx and t2g.txt are files generated by kb ref, and R1.fastq.gz and R2.fastq.gz are your raw paired-end reads. The --h5ad argument outputs an .h5ad file suitable for downstream processing with Scanpy.\nIf you performed a bulk nucleus RNA-seq experiment, generate your index with kb ref using the NAC workflow. Then, run kb count with the standard workflow as shown above."
  },
  {
    "objectID": "notebooks/replicate_pipeline.html",
    "href": "notebooks/replicate_pipeline.html",
    "title": "",
    "section": "",
    "text": "import anndata as ad\nimport scanpy as sc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nimport os\nfrom pathlib import Path\n\n\n\n\nconditions = [\"CTRL\", \"PM\"]\ngeo_accessions=[[\"GSM8385872\", \"GSM8385873\"], [\"GSM8385874\", \"GSM8385875\"]]\n\nurl_template = \"https://www.ncbi.nlm.nih.gov/geo/download/?acc={}&format=file&file={}%5F{}%5F{}%5F{}%2E{}%2Egz\"\n\ndef download_file(url, local_filename):\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(local_filename, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n    print(f\"Downloaded {local_filename}\")\n\nsample_adatas = []\nsample_names = []\nfor condition, condition_geos in zip(conditions, geo_accessions):\n    for i, geo_accession in enumerate(condition_geos):\n        mtx_url = url_template.format(geo_accession, geo_accession, condition, i+1, \"matrix\", \"mtx\")\n        barcode_url = url_template.format(geo_accession, geo_accession, condition, i+1, \"barcodes\", \"tsv\")\n        feature_url = url_template.format(geo_accession, geo_accession, condition, i+1, \"features\", \"tsv\")\n\n        sample_name = f\"{condition}_{i+1}\"\n        sample_path = os.path.join(\"data\", sample_name)\n\n        os.makedirs(sample_path , exist_ok = True)\n\n        mtx_path = os.path.join(sample_path, \"matrix.mtx.gz\")\n        barcode_path = os.path.join(sample_path, \"barcodes.tsv.gz\")\n        feature_path = os.path.join(sample_path, \"features.tsv.gz\")\n\n        if not (Path(mtx_path).exists() and Path(barcode_path).exists() and Path(feature_path).exists()):\n            download_file(mtx_url, mtx_path)\n            download_file(barcode_url, barcode_path)\n            download_file(feature_url, feature_path)\n\n        sample_adata = sc.read_10x_mtx(sample_path)\n        sample_adata.obs[\"sample\"] = sample_name\n        sample_adata.obs[\"condition\"] = condition\n\n        sample_adatas.append(sample_adata)\n        sample_names.append(sample_name)\n\n\ndef saturation_plot(adata):\n    # Create a plot showing genes detected as a function of UMI counts.\n    fig, ax = plt.subplots(figsize=(10, 7))\n\n    x = np.asarray(adata.X.sum(axis=1))[:,0]\n    y = np.asarray(np.sum(adata.X&gt;0, axis=1))[:,0]\n\n    ax.scatter(x, y, color=\"green\", alpha=0.25)\n    ax.set_xlabel(\"UMI Counts\")\n    ax.set_ylabel(\"Genes Detected\")\n    ax.set_xscale('log')\n    ax.set_yscale('log', nonpositive='clip')\n\n    ax.set_xlim((0.5, 4500))\n    ax.set_ylim((0.5, 2000))\n\n    plt.show()\n\ndef knee_plot(adata, umi_cutoff):\n    knee = np.sort((np.array(adata.X.sum(axis=1))).flatten())[::-1]\n    cell_set = np.arange(len(knee))\n    num_cells = cell_set[knee &gt; umi_cutoff][::-1][0]\n\n    fig, ax = plt.subplots(figsize=(10, 7))\n\n    ax.loglog(knee, cell_set, linewidth=5, color=\"g\")\n    ax.axvline(x=umi_cutoff, linewidth=3, color=\"k\")\n    ax.axhline(y=num_cells, linewidth=3, color=\"k\")\n\n    ax.set_xlabel(\"UMI Counts\")\n    ax.set_ylabel(\"Set of Barcodes\")\n\n    plt.grid(True, which=\"both\")\n    plt.show()\n\ndef quality_ctrl_plots(adata, umi_cutoff):\n    saturation_plot(adata)\n    knee_plot(adata, umi_cutoff)\n\numi_cutoffs = [100, 100, 100, 100]\nfor adata, umi_cutoff in zip(sample_adatas, umi_cutoffs):\n    saturation_plot(adata)\n    knee_plot(adata, umi_cutoff)"
  },
  {
    "objectID": "pages/HPC.html",
    "href": "pages/HPC.html",
    "title": "Introduction to the Resnick High Performance Computing Center (HPC)",
    "section": "",
    "text": "Genomics data produces hundreds of thousands of counts from thousands of cells. A standard experiment produces several gigabytes of raw reads, too many for your personal computer to store and process efficiently. the Resnick High Performance Computing Center (HPC) at Caltech. Here, we provide a tutorial emphasizing the use of the HPC for the analysis of genomics data. For a more general tutorial, please refer to the Caltech HPC documentation.\nUnlike your personal computer, the HPC is a shared computational environment designed to support hundreds of users and thousands of jobs simultaneously.\nUsing the HPC is very different from working on a laptop:\n\nThere is no graphical user interface (GUI)\nMost work is done from the command line via SSH\nLarge computational tasks must be submitted to a job scheduler rather than run directly\n\nYou can access the HPC in two main ways:\n\nThe command-line\nThe web-based interactive portal\n\n\n\n\n\nTo sign into the HPC from the command line, open your terminal and type:\n    ssh username@login.hpc.caltech.edu\nThe command line will prompt you for your password. After you enter your password, enter 1 to verify your login with DUO two-factor identification.\nWhen you log in from the terminal, you are connecting to a login node. Login nodes are shared machines intended for:\n\nEditing files\nSubmitting jobs\nManaging data\nLoading software modules\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should not run large bioinformatics analyses directly on the login node. Instead, computational jobs must be submitted to the scheduler (SLURM), which will run them on dedicated compute nodes.\n\n\n\n\n\nYour terminal (also called a shell) is a text-based interface that allows you to communicate directly with a computer’s operating system.\nThe Caltech HPC runs the Linux operating system. Most users interact with Linux through a shell called Bash (Bourne Again SHell).\nA typical Bash command follows this general structure:\ncommand [options] [arguments]\nFor example, a standard kb-python command might look like:\nkb -x 10XV4 read1.fastq.gz read2.fastq.gz\nIn this case:\n\nkb is the command (the program being executed)\n-x 10XV4 is an option (or flag) modifying how the program runs\nread1.fastq.gz and read2.fastq.gz are input files\n\n\n\n\nIf you plan to work through the command line, here are the minimum commands you should be comfortable with:\nNavigating the Filesystem\n\npwd — Print your current working directory\nls — List files in the current directory\nls -lh — List files with human-readable sizes\ncd directory_name — Change directories\ncd .. — Move up one directory\n\nNote that there are two types of file paths:\n\nrelative: the path relative to the current working directory\nabsolute: the path starting from the base of the working tree\n\nLet’s consider of an image with the path ~/Desktop/images/test.png. Say that you navigate to the directory ~/Desktop. You may reference the file either through:\n\nthe relative path: images/test.png\nthe absolute path: ~/Desktop/images/test.png\n\nWhether you choose to use the relative or absolute path depends upon your exact intentions. For instance, if you want to reference a file outside your current directory, you must use the absolute path. Understanding file paths is extremely important to working with the command line.\nManaging Files\n\nmv file newname — Move or rename a file\ncp file destination — Copy a file\nrm file — Remove a file (permanently)\nmkdir new_directory — Create a directory\n\nViewing Large Files\nGenomics files can be tens or hundreds of gigabytes. You should never try to open them in a graphical editor.\nInstead, use:\n\nless file.txt — Scroll through a file\nhead file.txt — View the first 10 lines\ntail file.txt — View the last 10 lines\n\nThese commands can be useful for quickly verifying the format of FASTQ and SAM files.\nIf your files are zipped, you can also peek into them\n\n\n\n\nThe HPC is a shared resource. To ensure fair and efficient usage, computational jobs must be submitted to a scheduling system rather than executed immediately.\nCaltech’s HPC uses Simple Linux Utility for Resource Management (SLURM) to: - Allocate CPUs, memory, and GPUs - Manage job queues - Schedule jobs based on priority and availability - Monitor running jobs - Handle parallel and distributed computing tasks\nThe means, instead of running a command or script directly, you will typically place your command inside a job submission script and submit it to SLURM.\n\n\n\nIf you would like to avoid working form the command-line, the HPC offers an Open OnDemand portal, a browser interface with the HPC.\nFrom the Open OnDemand portal, you can: - Upload, download, move and delete files and folders through the web browser. - Edit and save files. - Open a command line shell. - View up to date details of jobs pending or running on the cluster. - Submit jobs from the web console using preset templates. -"
  },
  {
    "objectID": "pages/HPC.html#interfacing-with-hpc-through-the-command-line",
    "href": "pages/HPC.html#interfacing-with-hpc-through-the-command-line",
    "title": "Introduction to the Resnick High Performance Computing Center (HPC)",
    "section": "",
    "text": "To sign into the HPC from the command line, open your terminal and type:\n    ssh username@login.hpc.caltech.edu\nThe command line will prompt you for your password. After you enter your password, enter 1 to verify your login with DUO two-factor identification.\nWhen you log in from the terminal, you are connecting to a login node. Login nodes are shared machines intended for:\n\nEditing files\nSubmitting jobs\nManaging data\nLoading software modules\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should not run large bioinformatics analyses directly on the login node. Instead, computational jobs must be submitted to the scheduler (SLURM), which will run them on dedicated compute nodes.\n\n\n\n\n\nYour terminal (also called a shell) is a text-based interface that allows you to communicate directly with a computer’s operating system.\nThe Caltech HPC runs the Linux operating system. Most users interact with Linux through a shell called Bash (Bourne Again SHell).\nA typical Bash command follows this general structure:\ncommand [options] [arguments]\nFor example, a standard kb-python command might look like:\nkb -x 10XV4 read1.fastq.gz read2.fastq.gz\nIn this case:\n\nkb is the command (the program being executed)\n-x 10XV4 is an option (or flag) modifying how the program runs\nread1.fastq.gz and read2.fastq.gz are input files\n\n\n\n\nIf you plan to work through the command line, here are the minimum commands you should be comfortable with:\nNavigating the Filesystem\n\npwd — Print your current working directory\nls — List files in the current directory\nls -lh — List files with human-readable sizes\ncd directory_name — Change directories\ncd .. — Move up one directory\n\nNote that there are two types of file paths:\n\nrelative: the path relative to the current working directory\nabsolute: the path starting from the base of the working tree\n\nLet’s consider of an image with the path ~/Desktop/images/test.png. Say that you navigate to the directory ~/Desktop. You may reference the file either through:\n\nthe relative path: images/test.png\nthe absolute path: ~/Desktop/images/test.png\n\nWhether you choose to use the relative or absolute path depends upon your exact intentions. For instance, if you want to reference a file outside your current directory, you must use the absolute path. Understanding file paths is extremely important to working with the command line.\nManaging Files\n\nmv file newname — Move or rename a file\ncp file destination — Copy a file\nrm file — Remove a file (permanently)\nmkdir new_directory — Create a directory\n\nViewing Large Files\nGenomics files can be tens or hundreds of gigabytes. You should never try to open them in a graphical editor.\nInstead, use:\n\nless file.txt — Scroll through a file\nhead file.txt — View the first 10 lines\ntail file.txt — View the last 10 lines\n\nThese commands can be useful for quickly verifying the format of FASTQ and SAM files.\nIf your files are zipped, you can also peek into them"
  },
  {
    "objectID": "pages/HPC.html#scheduling-a-job-with-slurm",
    "href": "pages/HPC.html#scheduling-a-job-with-slurm",
    "title": "Introduction to the Resnick High Performance Computing Center (HPC)",
    "section": "",
    "text": "The HPC is a shared resource. To ensure fair and efficient usage, computational jobs must be submitted to a scheduling system rather than executed immediately.\nCaltech’s HPC uses Simple Linux Utility for Resource Management (SLURM) to: - Allocate CPUs, memory, and GPUs - Manage job queues - Schedule jobs based on priority and availability - Monitor running jobs - Handle parallel and distributed computing tasks\nThe means, instead of running a command or script directly, you will typically place your command inside a job submission script and submit it to SLURM."
  },
  {
    "objectID": "pages/HPC.html#section",
    "href": "pages/HPC.html#section",
    "title": "Introduction to the Resnick High Performance Computing Center (HPC)",
    "section": "",
    "text": "If you would like to avoid working form the command-line, the HPC offers an Open OnDemand portal, a browser interface with the HPC.\nFrom the Open OnDemand portal, you can: - Upload, download, move and delete files and folders through the web browser. - Edit and save files. - Open a command line shell. - View up to date details of jobs pending or running on the cluster. - Submit jobs from the web console using preset templates. -"
  }
]